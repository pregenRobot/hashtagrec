{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Hashtag Recommender - Analysis\n",
    "\n",
    "\n",
    "After a day of mining, tweepy was able to collect 79195 tweets and converted it to a 50 dimension vector (details of this is stated in the Presentation_mining.ipynb) file\n",
    "\n",
    "### Reading the mined data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79195, 50)\n",
      "(79195, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open(\"mined_data/data_X.csv\",\"rt\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        X = list(reader)\n",
    "        \n",
    "with open(\"mined_data/data_y.csv\",\"rt\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        y = list(reader)\n",
    "        \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tweets per hashtag however, was not consistent. During the mining process, a maximum of 1000 tweets per hashtag was set, but as you can see, hashtags such as \"girl\" only contained 16 tweets. A Stratified Shuffle was generated to separate the training data and test data to have equal proportions of hashtags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'#beach': 1000, '#family': 1000, '#friend': 811, '#instagood': 1000, '#love': 1000, '#photooftheday': 1000, '#yellow': 1000, '#fashion': 1000, '#beautiful': 272, '#happy': 1000, '#cute': 233, '#tbt': 1000, '#picoftheday': 1000, '#follow': 1000, '#selfie': 1000, '#summer': 1000, '#art': 1000, '#instadaily': 1000, '#nature': 1000, '#girl': 16, '#fun': 1000, '#style': 1000, '#smile': 1000, '#food': 758, '#instalike': 1000, '#likeforlike': 1000, '#fitness': 529, '#igers': 1000, '#tagsforlikes': 1000, '#nofilter': 1000, '#life': 1000, '#beauty': 292, '#amazing': 1000, '#instagram': 1000, '#photography': 1000, '#vscocam': 1000, '#photo': 1000, '#sun': 1000, '#music': 1000, '#ootd': 1000, '#bestoftheday': 88, '#sunset': 1000, '#sky': 806, '#dog': 1000, '#vsco': 1000, '#makeup': 1000, '#foodporn': 683, '#hair': 1000, '#pretty': 808, '#cat': 1000, '#model': 874, '#swag': 996, '#motivation': 1000, '#baby': 1000, '#party': 548, '#cool': 1000, '#gym': 618, '#lol': 1000, '#design': 1000, '#instapic': 1000, '#funny': 1000, '#healthy': 1000, '#night': 1000, '#lifestyle': 1000, '#yummy': 1000, '#tflers': 1000, '#instafood': 276, '#handmade': 1000, '#fit': 531, '#christmas': 1000, '#black': 231, '#blue': 1000, '#workout': 154, '#work': 1000, '#blackandwhite': 1000, '#drawing': 1000, '#holiday': 1000, '#london': 1000, '#sea': 1000, '#instacool': 1000, '#goodmorning': 1000, '#iphoneonly': 671, '#blessed': 1000, '#red': 1000, '#dogsofinstagram': 1000, '#throwback': 1000, '#happiness': 1000, '#instalove': 1000, '#coffee': 1000}\n"
     ]
    }
   ],
   "source": [
    "labeloccurence = {}\n",
    "\n",
    "for label in y.flatten():\n",
    "    try:\n",
    "        labeloccurence[label] = labeloccurence[label] + 1\n",
    "    except:\n",
    "        labeloccurence[label] = 1\n",
    "        \n",
    "        \n",
    "print(labeloccurence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5557</th>\n",
       "      <td>1.8970260387286544</td>\n",
       "      <td>11.426142051815987</td>\n",
       "      <td>2.286433018743992</td>\n",
       "      <td>3.6580030396580696</td>\n",
       "      <td>-3.2866700291633606</td>\n",
       "      <td>-4.1619668528437614</td>\n",
       "      <td>-9.547938160132617</td>\n",
       "      <td>-6.340730000287294</td>\n",
       "      <td>1.053048822504934</td>\n",
       "      <td>-4.044937053695321</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1010588519275188</td>\n",
       "      <td>6.0685998648405075</td>\n",
       "      <td>-6.114979052916169</td>\n",
       "      <td>5.338927154429257</td>\n",
       "      <td>-1.7078100219368935</td>\n",
       "      <td>3.6886659651063383</td>\n",
       "      <td>-0.4430071637034416</td>\n",
       "      <td>3.8419770002365112</td>\n",
       "      <td>0.18545900285243988</td>\n",
       "      <td>#photooftheday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51867</th>\n",
       "      <td>6.770628258673241</td>\n",
       "      <td>5.390256021171808</td>\n",
       "      <td>7.479233972728252</td>\n",
       "      <td>1.7166554885916412</td>\n",
       "      <td>1.4513099640607834</td>\n",
       "      <td>-1.4184269718825817</td>\n",
       "      <td>-3.5620199386030436</td>\n",
       "      <td>-7.850565972737968</td>\n",
       "      <td>-2.35472735442454</td>\n",
       "      <td>-1.7609331631101668</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.59414797462523</td>\n",
       "      <td>0.832190650049597</td>\n",
       "      <td>1.4485529512166977</td>\n",
       "      <td>-2.9203838554385584</td>\n",
       "      <td>6.806337898597121</td>\n",
       "      <td>2.2226188513450325</td>\n",
       "      <td>4.469500884413719</td>\n",
       "      <td>6.190316407941282</td>\n",
       "      <td>1.2796433912590146</td>\n",
       "      <td>#design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29334</th>\n",
       "      <td>-0.8846498653292656</td>\n",
       "      <td>4.359336979687214</td>\n",
       "      <td>-0.5777599550783634</td>\n",
       "      <td>5.230512024834752</td>\n",
       "      <td>-1.4255489446222782</td>\n",
       "      <td>-6.98695595562458</td>\n",
       "      <td>-4.2811000645160675</td>\n",
       "      <td>-5.169744022190571</td>\n",
       "      <td>-4.81668734527193</td>\n",
       "      <td>6.982820004224777</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2451081599574536</td>\n",
       "      <td>-3.6353960260748863</td>\n",
       "      <td>-6.352880030870438</td>\n",
       "      <td>-3.0788000524044037</td>\n",
       "      <td>-5.381459020078182</td>\n",
       "      <td>1.385561030358076</td>\n",
       "      <td>-3.0108290389180183</td>\n",
       "      <td>7.013682007789612</td>\n",
       "      <td>7.76913595572114</td>\n",
       "      <td>#instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44266</th>\n",
       "      <td>4.408114038407803</td>\n",
       "      <td>0.8948999792337418</td>\n",
       "      <td>4.255405901814811</td>\n",
       "      <td>-2.9145560916513205</td>\n",
       "      <td>-1.1366247844416648</td>\n",
       "      <td>-2.2644220776855946</td>\n",
       "      <td>-4.2542518600821495</td>\n",
       "      <td>-8.765686988830566</td>\n",
       "      <td>-1.8110286509618163</td>\n",
       "      <td>1.9321849904954433</td>\n",
       "      <td>...</td>\n",
       "      <td>1.478124056942761</td>\n",
       "      <td>0.11545391380786896</td>\n",
       "      <td>-0.44217707216739655</td>\n",
       "      <td>1.8928942400962114</td>\n",
       "      <td>0.5045299828052521</td>\n",
       "      <td>-0.08957099169492722</td>\n",
       "      <td>-5.362092062830925</td>\n",
       "      <td>2.5468911081552505</td>\n",
       "      <td>2.0169109888374805</td>\n",
       "      <td>#cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10606</th>\n",
       "      <td>-1.6573240086436272</td>\n",
       "      <td>5.77875205129385</td>\n",
       "      <td>-1.9465200901031494</td>\n",
       "      <td>0.16022955905646086</td>\n",
       "      <td>-6.251160055398941</td>\n",
       "      <td>-4.399850085377693</td>\n",
       "      <td>6.2419000416994095</td>\n",
       "      <td>-5.1330640241503716</td>\n",
       "      <td>-2.443369960412383</td>\n",
       "      <td>5.6650701850885525</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.089522950351238</td>\n",
       "      <td>-0.7252710424363613</td>\n",
       "      <td>-3.160119991749525</td>\n",
       "      <td>0.5469664859992918</td>\n",
       "      <td>-0.5970050133764744</td>\n",
       "      <td>1.2338529713451862</td>\n",
       "      <td>0.9109479617327452</td>\n",
       "      <td>7.015899032354355</td>\n",
       "      <td>8.603660078719258</td>\n",
       "      <td>#picoftheday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0                   1                    2  \\\n",
       "5557    1.8970260387286544  11.426142051815987    2.286433018743992   \n",
       "51867    6.770628258673241   5.390256021171808    7.479233972728252   \n",
       "29334  -0.8846498653292656   4.359336979687214  -0.5777599550783634   \n",
       "44266    4.408114038407803  0.8948999792337418    4.255405901814811   \n",
       "10606  -1.6573240086436272    5.77875205129385  -1.9465200901031494   \n",
       "\n",
       "                         3                    4                    5  \\\n",
       "5557    3.6580030396580696  -3.2866700291633606  -4.1619668528437614   \n",
       "51867   1.7166554885916412   1.4513099640607834  -1.4184269718825817   \n",
       "29334    5.230512024834752  -1.4255489446222782    -6.98695595562458   \n",
       "44266  -2.9145560916513205  -1.1366247844416648  -2.2644220776855946   \n",
       "10606  0.16022955905646086   -6.251160055398941   -4.399850085377693   \n",
       "\n",
       "                         6                    7                    8  \\\n",
       "5557    -9.547938160132617   -6.340730000287294    1.053048822504934   \n",
       "51867  -3.5620199386030436   -7.850565972737968    -2.35472735442454   \n",
       "29334  -4.2811000645160675   -5.169744022190571    -4.81668734527193   \n",
       "44266  -4.2542518600821495   -8.765686988830566  -1.8110286509618163   \n",
       "10606   6.2419000416994095  -5.1330640241503716   -2.443369960412383   \n",
       "\n",
       "                         9  ...                   41                   42  \\\n",
       "5557    -4.044937053695321  ...  -1.1010588519275188   6.0685998648405075   \n",
       "51867  -1.7609331631101668  ...    -3.59414797462523    0.832190650049597   \n",
       "29334    6.982820004224777  ...   2.2451081599574536  -3.6353960260748863   \n",
       "44266   1.9321849904954433  ...    1.478124056942761  0.11545391380786896   \n",
       "10606   5.6650701850885525  ...   -4.089522950351238  -0.7252710424363613   \n",
       "\n",
       "                         43                   44                   45  \\\n",
       "5557     -6.114979052916169    5.338927154429257  -1.7078100219368935   \n",
       "51867    1.4485529512166977  -2.9203838554385584    6.806337898597121   \n",
       "29334    -6.352880030870438  -3.0788000524044037   -5.381459020078182   \n",
       "44266  -0.44217707216739655   1.8928942400962114   0.5045299828052521   \n",
       "10606    -3.160119991749525   0.5469664859992918  -0.5970050133764744   \n",
       "\n",
       "                         46                   47                  48  \\\n",
       "5557     3.6886659651063383  -0.4430071637034416  3.8419770002365112   \n",
       "51867    2.2226188513450325    4.469500884413719   6.190316407941282   \n",
       "29334     1.385561030358076  -3.0108290389180183   7.013682007789612   \n",
       "44266  -0.08957099169492722   -5.362092062830925  2.5468911081552505   \n",
       "10606    1.2338529713451862   0.9109479617327452   7.015899032354355   \n",
       "\n",
       "                        49             tag  \n",
       "5557   0.18545900285243988  #photooftheday  \n",
       "51867   1.2796433912590146         #design  \n",
       "29334     7.76913595572114      #instagram  \n",
       "44266   2.0169109888374805            #cat  \n",
       "10606    8.603660078719258    #picoftheday  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1,random_state=13,test_size=0.2)\n",
    "column_labels = list(range(50))\n",
    "column_labels = [str(i) for i in column_labels]\n",
    "column_labels.append(\"tag\")\n",
    "data = np.hstack((X,y))\n",
    "dataDF = pd.DataFrame(data,columns=column_labels)\n",
    "\n",
    "for train_index, test_index in splitter.split(dataDF,dataDF[\"tag\"]):\n",
    "    stratified_train = dataDF.loc[train_index]\n",
    "    stratified_test = dataDF.loc[test_index]\n",
    "    \n",
    "X_train = stratified_train[column_labels[:-1]]\n",
    "y_train = stratified_train[\"tag\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "stratified_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we will work on training with a scaled version of the data data (X_train_scaled) using the various multi-class classifiers supported by scikit-learn. Training with OneVsOne classifiers was skipped because of the large dimensionality of the training data and certain classifiers such as `GaussianProcessClassifier()` was skipped because the large sample data made the training process extremely inefficient due to the training machine's ram.\n",
    "\n",
    "Here is a rundown of their recall,accuracy, an calculated with a 3-way cross validation.\n",
    "\n",
    "\n",
    "#### Inherently Multiclass\n",
    "\n",
    "Table 1\n",
    "\n",
    "|Classifier|Precision|Recall|F1-Score|\n",
    "|:-|-|-|-|\n",
    "|`BernoulliNB()`|0.17871032497557543|0.16760843487593913|0.15193282169569067|\n",
    "|`DecisionTreeClassifier()`|0.32884656073391016|0.3284140412904855|0.32775179443622027|\n",
    "|`ExtraTreeClassifier()`|0.3170519558192891|0.31613422564555843|0.31569708377813743|\n",
    "|`ExtraTreesClassifier()`|0.45201342059650934|0.44396742218574403|0.4408797517858572|\n",
    "|`GaussianNB()`|0.19586086737595293|0.14434307721447062|0.13305816692082342|\n",
    "|`KNeighborsClassifier()`|0.4094528189643595|0.38569354125891786|0.38707707881014963|\n",
    "|`LinearDiscriminantAnalysis()`|0.29787034359887066|0.3069006881747585|0.2926308049269754|\n",
    "|`LinearSVC(multi_class=\"crammer_singer\")`|0.09890950041654863|0.10818233474335501|0.0828675811060722|\n",
    "|`LogisticRegression(multi_class=\"multinomial\")`|0.31802802668700164|0.3395889892038639|0.3222431373823199|\n",
    "|`NearestCentroid()`|0.19837361321709462|0.16797146284487657|0.15873458662002415|\n",
    "|`QuadraticDiscriminantAnalysis()`|0.3423328288077747|0.32120083338594607|0.3188687539502235|\n",
    "|`RandomForestClassifier()`|0.4456224929103627|0.4420891470421112|0.4375279167445373|\n",
    "|`RidgeClassifier()`|0.2940304427404206|0.288370477934213|0.2404565802969223|\n",
    "\n",
    "#### OneVsRest\n",
    "\n",
    "Table2\n",
    "\n",
    "| Classifier | Recall| Accuracy |F1-Score|\n",
    "|:---------- | --------| ---------|-----------|\n",
    "|`SGDClassifier()`|0.21791900968005287|0.2065155628511901|0.2050632340660742|\n",
    "|`Perceptron()`|0.17013810779919047|0.1521245028095208|0.15134008501248902|\n",
    "|`PassiveAggressiveClassifier()`|0.1793218678089453|0.166140539175453|0.1617584027624504|\n",
    "|`GradientBoostingClassifier()`|0.36644844360010204|0.3708093945324831|0.36606015391948754|\n",
    "|`LinearSVC()`|0.27899933452956677|0.31411389607929796|0.26547331622756415|\n",
    "|`LogisticRegression(multi_class=\"ovr\")`|0.3068813252069861|0.33109729149567524|0.30878692718362605|\n",
    "\n",
    "It should also be noted that both LogisticRegression and LinearSVC failed to converge.\n",
    "QuadraticDiscriminantAnalysis threw a runtime warning that variables were collinear\n",
    "\n",
    "At a glance, it looks like the Ensemble, Decision-Tree, and KNieghbors are doing relatively well. We will go deeper into training with these algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ExtraTreesClassifier\n",
    "\n",
    "The recalls and precisions from the classifiers trained with X_train_scaled and tested against y_train were around 88%, which suggests the classifiers are overfitting. Changing the criterion from \"gini\" to \"entropy\" did not help either\n",
    "\n",
    "The overfitted-classifier had tree-depths between 35 and 46, and had approximately 30000 leaves. To regularize this, A Grid Search was carried out to find the best hyper parameters fro `ExtraTreesClassifier` that caps the maximum tree-depth at 30 and maximum leaf-node at 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 41, 38, 39, 41, 42, 39, 42, 39, 43, 39, 38, 45, 40, 39, 38, 40, 35, 39, 37, 41, 43, 38, 40, 40, 43, 44, 39, 39, 46, 42, 47, 41, 41, 38, 41, 41, 38, 40, 39, 39, 44, 43, 40, 40, 42, 36, 45, 42, 40, 38, 44, 39, 44, 39, 42, 46, 38, 41, 42, 43, 45, 45, 38, 40, 41, 40, 39, 41, 44, 44, 40, 44, 40, 39, 43, 40, 40, 39, 45, 43, 41, 47, 45, 42, 38, 41, 46, 39, 39, 41, 39, 41, 38, 48, 40, 44, 49, 47, 42]\n",
      "[33964, 34059, 33952, 34114, 34115, 34113, 34217, 34075, 34049, 34057, 34137, 34118, 33950, 33977, 33957, 34078, 34106, 34020, 34100, 34166, 33965, 33989, 34045, 34102, 33960, 34038, 33981, 34092, 33946, 34112, 33971, 34087, 34066, 34027, 34077, 34020, 33999, 34020, 34029, 34004, 34018, 33992, 33895, 34034, 33988, 33882, 33961, 34107, 33963, 34045, 33931, 34069, 34051, 34004, 34076, 34000, 33991, 34010, 34113, 34104, 33949, 33974, 34024, 33997, 34150, 34024, 33944, 33963, 34025, 34016, 34082, 34093, 34034, 33977, 34089, 33950, 34076, 34069, 34084, 34162, 34037, 34058, 33971, 33931, 33923, 33905, 34085, 33998, 34115, 33980, 33971, 33996, 34143, 33999, 34077, 34085, 33984, 34018, 34018, 33791]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etreesclf = ExtraTreesClassifier()\n",
    "\n",
    "etreesclf.fit(X_train_scaled,y_train)\n",
    "\n",
    "print([estimator.get_depth() for estimator in etreesclf.estimators_])\n",
    "\n",
    "print([estimator.get_n_leaves() for estimator in etreesclf.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 30,\n",
       " 'max_leaf_nodes': 20000,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 10}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{\"max_depth\":[None,10,20,30],\n",
    "               \"min_samples_split\":[2,10,20,30,40],\n",
    "               \"min_samples_leaf\":[1,20,40,60],\n",
    "               \"max_leaf_nodes\":[None,100,10000,20000,30000]\n",
    "              }]\n",
    "\n",
    "grid_search = GridSearchCV(etreesclf, param_grid, cv=3, scoring='f1_weighted')\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4417280526270308"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this is `ExtraTreesClassifier`'s best performance. We will try repeating the process for the next most successful classifier: `RandomForestClassifier`\n",
    "\n",
    "This classifier also was overfitting with an f1 score of 0.8712136016947981, calculated by the prediction based on X_train_scaled and compared against y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 61, 57, 58, 61, 57, 61, 65, 61, 56, 72, 57, 67, 59, 54, 59, 66, 60, 63, 62, 57, 58, 57, 65, 63, 61, 53, 61, 59, 52, 61, 58, 55, 57, 59, 63, 59, 53, 56, 69, 61, 56, 58, 63, 63, 64, 74, 51, 60, 62, 57, 56, 57, 58, 59, 58, 62, 63, 65, 63, 66, 58, 66, 67, 59, 60, 70, 66, 68, 62, 63, 60, 53, 64, 58, 61, 61, 59, 62, 61, 59, 65, 65, 53, 61, 83, 67, 55, 53, 62, 64, 62, 56, 58, 58, 69, 61, 57, 60, 62]\n",
      "[20594, 20658, 20534, 20599, 20627, 20483, 20733, 20601, 20646, 20605, 20704, 20625, 20391, 20614, 20636, 20650, 20701, 20647, 20536, 20649, 20565, 20553, 20666, 20579, 20593, 20694, 20561, 20633, 20571, 20561, 20537, 20668, 20658, 20495, 20558, 20438, 20545, 20616, 20671, 20656, 20457, 20588, 20588, 20583, 20610, 20604, 20595, 20596, 20526, 20518, 20460, 20734, 20452, 20623, 20548, 20608, 20433, 20536, 20516, 20613, 20484, 20602, 20465, 20571, 20569, 20769, 20511, 20648, 20671, 20588, 20619, 20469, 20577, 20531, 20509, 20552, 20426, 20656, 20545, 20452, 20591, 20597, 20642, 20562, 20472, 20598, 20641, 20434, 20567, 20561, 20623, 20537, 20493, 20404, 20488, 20506, 20513, 20580, 20643, 20532]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randforclf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "randforclf.fit(X_train_scaled,y_train)\n",
    "\n",
    "print([estimator.get_depth() for estimator in randforclf.estimators_])\n",
    "\n",
    "print([estimator.get_n_leaves() for estimator in randforclf.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 600 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 28.3min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 72.6min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 195.3min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 383.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 551.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1560 tasks      | elapsed: 677.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed: 757.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'max_leaf_nodes': 10000, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.43954581494967454\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = [{\"max_depth\":[None,20,30,40,50],\n",
    "               \"min_samples_split\":[2,10,20,30,40],\n",
    "               \"min_samples_leaf\":[1,20,40,60],\n",
    "               \"max_leaf_nodes\":[None,100,5000,10000,15000,20000]\n",
    "              }]\n",
    "\n",
    "grid_search_rf = GridSearchCV(randforclf, param_grid_rf, cv=3, scoring='f1_weighted',verbose=3,n_jobs=-1)\n",
    "\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like tuning the hyperparameter does not help the classifier from overfitting, at all. However, we can certainly say that ensemble methods have been the most effective classification algorithms from the results in Table 1 and 2. Here, we will try increasing the variation of the prediciton by introducting `VotingClassifier` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "logregclf_mnml = LogisticRegression(multi_class=\"multinomial\")\n",
    "logregclf_ovr = LogisticRegression(multi_class=\"ovr\")\n",
    "extratree_clf = ExtraTreeClassifier()\n",
    "randforrest_clf = RandomForestClassifier(\n",
    "    max_depth=50,\n",
    "    max_leaf_nodes=10000,\n",
    "    min_samples_leaf=1, \n",
    "    min_samples_split= 2\n",
    ")\n",
    "extratrees_clf = ExtraTreesClassifier(\n",
    "    max_depth=30,\n",
    "    max_leaf_nodes=20000,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10\n",
    ")\n",
    "\n",
    "kneighbors_clf = KNeighborsClassifier()\n",
    "qddicranalysis_clf = QuadraticDiscriminantAnalysis()\n",
    "ridge_clf = RidgeClassifier()\n",
    "\n",
    "\n",
    "voting_classifer_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr_mnml\",logregclf_mnml),\n",
    "        (\"lr_ovr\",logregclf_ovr),\n",
    "        (\"tree\",extratree_clf),\n",
    "        (\"trees\",extratrees_clf),\n",
    "        (\"forrest\",randforrest_clf),\n",
    "        (\"knn\",kneighbors_clf),\n",
    "        (\"quad\",qddicranalysis_clf),\n",
    "        (\"ridge\",ridge_clf)\n",
    "    ],\n",
    "    voting=\"hard\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "voting_classifer_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"lr_mnml\",logregclf_mnml),\n",
    "        (\"lr_ovr\",logregclf_ovr),\n",
    "        (\"tree\",extratree_clf),\n",
    "        (\"trees\",extratrees_clf),\n",
    "        (\"forrest\",randforrest_clf),\n",
    "        (\"knn\",kneighbors_clf),\n",
    "        (\"quad\",qddicranalysis_clf),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_pred_hard = cross_val_predict(voting_classifer_hard,X_train_scaled,y_train,cv=3)\n",
    "voting_pred_soft = cross_val_predict(voting_classifer_soft,X_train_scaled,y_train,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting:\n",
      "\n",
      "    Accuracy: 0.4642496369720311\n",
      "    Recall: 0.4642496369720311\n",
      "    F1-score: 0.45077807939330994\n",
      "\n",
      "Soft Voting:\n",
      "\n",
      "    Accuracy: 0.42748910916093186\n",
      "    Recall: 0.42748910916093186\n",
      "    F1-score: 0.42268612744329837\n"
     ]
    }
   ],
   "source": [
    "print(\"Hard Voting:\\n\")\n",
    "print(\"    Accuracy: \" + str(accuracy_score(y_train,voting_pred_hard)))\n",
    "print(\"    Recall: \" + str(recall_score(y_train,voting_pred_hard,average=\"weighted\")))\n",
    "print(\"    F1-score: \" + str(f1_score(y_train,voting_pred_hard,average=\"weighted\")))\n",
    "print(\"\\nSoft Voting:\\n\")\n",
    "print(\"    Accuracy: \" + str(accuracy_score(y_train,voting_pred_soft)))\n",
    "print(\"    Recall: \" + str(recall_score(y_train,voting_pred_soft,average=\"weighted\")))\n",
    "print(\"    F1-score: \" + str(f1_score(y_train,voting_pred_soft,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it is going to be extremely difficult to even hit an f1 score larger than 0.5. Let's try one last time testing with boosting ensemble methods before moving on to testing our classifier on the final training set and creating a different dataset. We have already testing GradientBoostingClassifier (scores are displayed in Table 2), so let's try AdaBoost instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.44721889008144455\n",
      "Recall: 0.44721889008144455\n",
      "F1-score: 0.4494272009117301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=20000), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "\n",
    "ada_boost_pred = cross_val_predict(ada_clf,X_train_scaled, y_train,cv=3)\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(y_train,ada_boost_pred)))\n",
    "print(\"Recall: \" + str(recall_score(y_train,ada_boost_pred,average=\"weighted\")))\n",
    "print(\"F1-score: \" + str(f1_score(y_train,ada_boost_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like achieving an f1-score higher than 0.5 is going to be extremely difficult with this dataset. One of the possible reasons for such a low score is because the hashtags see to be correlated in some way. For example, we can imagine #happy,#cute, and #picoftheday in the same tweet.\n",
    "\n",
    "Our miner should have either:\n",
    "1.) Removed all the other hashtags aside from the tag we are querying with.\n",
    "2.) Created a mulit-label dataset.\n",
    "\n",
    "Another possible reason is because the majority of possible hashtags themselves seem to be correlated with one another in the sense that they are all \"semantically positive.\" For the next dataset, the miner should have also had hashtags which are \"semantically negative.\"\n",
    "\n",
    "Let's do a final test on the training set using the classifier with the highest scores: Hard Voting Classifier comprised of:\n",
    "`LogisticRegression(multiclass=\"multinomial\")`\n",
    "\n",
    "`LogisticRegression(multiclass=\"ovr\")`\n",
    "\n",
    "`ExtraTreeClassifier()`\n",
    "\n",
    "`RandomForestClassifier(max_depth=50,max_leaf_nodes=10000,min_samples_leaf=1, min_samples_split= 2)`\n",
    "\n",
    "`ExtraTreesClassifier(max_depth=30,max_leaf_nodes=20000,min_samples_leaf=1,min_samples_split=10)`\n",
    "\n",
    "`KNeighborsClassifier()`\n",
    "\n",
    "`QuadraticDiscriminantAnalysis()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr_mnml',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='multinomial',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('lr_ovr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fi...\n",
       "                                                   n_jobs=None, n_neighbors=5,\n",
       "                                                   p=2, weights='uniform')),\n",
       "                             ('quad',\n",
       "                              QuadraticDiscriminantAnalysis(priors=None,\n",
       "                                                            reg_param=0.0,\n",
       "                                                            store_covariance=False,\n",
       "                                                            tol=0.0001)),\n",
       "                             ('ridge',\n",
       "                              RidgeClassifier(alpha=1.0, class_weight=None,\n",
       "                                              copy_X=True, fit_intercept=True,\n",
       "                                              max_iter=None, normalize=False,\n",
       "                                              random_state=None, solver='auto',\n",
       "                                              tol=0.001))],\n",
       "                 flatten_transform=True, n_jobs=-1, voting='hard',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_classifer_hard.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 0.4836163899236063\n",
      "    Recall: 0.4836163899236063\n",
      "    F1-score: 0.4711353166334938\n"
     ]
    }
   ],
   "source": [
    "X_test = stratified_test[column_labels[:-1]]\n",
    "y_test = stratified_test[\"tag\"]\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "vc_pred = voting_classifer_hard.predict(X_test_scaled)\n",
    "\n",
    "print(\"    Accuracy: \" + str(accuracy_score(y_test,vc_pred)))\n",
    "print(\"    Recall: \" + str(recall_score(y_test,vc_pred,average=\"weighted\")))\n",
    "print(\"    F1-score: \" + str(f1_score(y_test,vc_pred,average=\"weighted\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "classifier_path = \"classifiers/\"\n",
    "final_clf = voting_classifer_hard\n",
    "\n",
    "with open(r\"classifiers/v1_classifier.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(final_clf, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now predict new tweets by running the following command at the project root.\n",
    "\n",
    "`python tweetrecommender.py \"Tweet text\" `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
