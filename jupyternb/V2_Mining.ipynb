{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:15.305470Z",
     "start_time": "2020-06-16T13:36:13.122890Z"
    }
   },
   "outputs": [],
   "source": [
    "from jupyterthemes import jtplot\n",
    "# jtplot.style(theme=\"gruvboxd\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T13:33:19.378636Z",
     "start_time": "2020-06-10T13:33:19.363086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      23424748\n",
       "1       1099805\n",
       "2       1100661\n",
       "3       1100968\n",
       "4       1101597\n",
       "         ...   \n",
       "105     2503713\n",
       "106     2503863\n",
       "107     2508428\n",
       "108     2512636\n",
       "109     2514815\n",
       "Name: woeid, Length: 110, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_trends = pd.read_csv(\"mined_data/available_trends.csv\")\n",
    "available_woeid = available_trends.iloc[:,5]\n",
    "available_woeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:29.011984Z",
     "start_time": "2020-06-16T13:36:28.842176Z"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = \"WwEbwqeW01Yy6WLj7OEQekl8W\"\n",
    "consumer_secret = \"phmcmDEfZyVz44f3MeVO9eLkbFsh6TPFe7qTWpfBTGYNJCZs9D\"\n",
    "access_token = \"1001251273981677568-iarpqVNKAQ28iDzcyU0QS7H50FCokR\"\n",
    "access_token_secret = \"BJoB0WH59h2vWejzyxDmunPQZFbEk0uN0dfoYFH1WGcf5\"\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:30.095908Z",
     "start_time": "2020-06-16T13:36:30.091502Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_woeid_details(woeid):\n",
    "    locations = api.trends_available()\n",
    "    \n",
    "    for i in range(len(locations)):\n",
    "        if(locations[i][\"woeid\"] == woeid):\n",
    "            return locations[i][\"name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T08:48:47.059879Z",
     "start_time": "2020-06-10T08:24:04.757898Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_of_hashtags = []\n",
    "for i in range(len(available_woeid)):\n",
    "    woeid = available_woeid[i]\n",
    "    trends = api.trends_place(woeid)[0][\"trends\"]\n",
    "    for ii in range(len(trends)):\n",
    "        trend = trends[ii][\"name\"]\n",
    "        if(trend[:1]==\"#\"):\n",
    "            list_of_hashtags.append(trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:21.653500Z",
     "start_time": "2020-06-16T13:36:21.434020Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_of_hashtags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a5e54767326c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlist_of_hashtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_hashtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mined_data/hashtags_200610.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_of_hashtags' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "list_of_hashtags = list(set(list_of_hashtags))\n",
    "\n",
    "with open('mined_data/hashtags_200610.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for val in list_of_hashtags: \n",
    "        writer.writerow([val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:36.502732Z",
     "start_time": "2020-06-16T13:36:36.487981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           #1ofTheMillion\n",
       "1                    #INwx\n",
       "2                    #newx\n",
       "3      #ChampionshipLeague\n",
       "4                  #hokies\n",
       "              ...         \n",
       "137                  #twug\n",
       "138                #Pompey\n",
       "139                #TikTok\n",
       "140                  #UTSA\n",
       "141       #shortlandstreet\n",
       "Name: #VoterSuppressionInGeoriga, Length: 142, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_hashtags = pd.read_csv(\"mined_data/hashtags_200610.csv\")\n",
    "available_hashtags.head()\n",
    "\n",
    "available_hashtags.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T09:06:35.582659Z",
     "start_time": "2020-06-17T09:06:28.957627Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "Retrieved 9 tweets about #1ofTheMillion\n",
      "Current num of newhashtags: 14\n",
      "['news', 'from', 'renishaw', ':', 'international', 'women', 'in', 'engineering', 'day', '2020', 'â€“', '1ofthemillion', '.', 'read', 'about', 'it', 'here', '-']\n",
      "['proudly', '1ofthemillion', '1mwis', 'campaign', 'to', 'celebrate', '1', 'million', 'women', 'working', 'in', 'stem', 'in', 'the', 'uk', 'womeninstem']\n",
      "['i', 'have', 'a', 'strong', 'blackintheivory', 'and', 'iit', 'class', 'of', '2020', 'and', 'am', 'proud', 'to', 'be', '1ofthemillion', 'womeninstem', 'winking', 'face']\n",
      "['we', 'work', 'to', 'inspire', 'the', 'next', 'generation', 'of', 'engineers', ',', 'academics', 'and', 'stem', 'specialists', 'canterburyccuni', ',', 'while', 'supporting', 'inclusivity', 'in', 'industry', 'inwed', '2020', 'thewisecampaign', '1ofthemillion']\n",
      "['for', 'the', 'first', 'time', 'ever', ',', 'there', 'are', 'more', 'than', '1', 'million', 'women', 'working', 'in', 'core', 'stem', 'roles', 'across', 'the', 'uk', 'join', 'thewisecampaign', 'as', 'they', 'celebrate', 'this', 'significant', 'milestone', 'and', 'inspire', 'more', 'women', 'to', 'become', '1ofthemillion']\n",
      "['great', 'to', 'see', 'this', 'milestone', 'reached', 'in', 'the', 'uk', 'and', 'so', 'proud', 'of', 'all', 'my', '1ofthemillion', 'friends', 'and', 'colleagues', '.', \"let's\", 'keep', 'up', 'the', 'progress', 'towards', 'using', 'all', 'of', 'the', 'human', 'talent', 'pool', 'to', 'do', 'the', 'best', 'possible', 'science', '.']\n",
      "['the', 'bob_lectures', 'showcase', 'the', 'best', 'of', 'bristoluni', 'lecturers', '&', 'this', 'week', \"we're\", 'celebrating', '1ofthemillion', 'women', 'working', 'in', 'stem', 'with', 'dr', 'lynne', 'walling', 'â€™', 's', 'talk', \"'\", 'the', 'art', '&', 'beauty', 'of', 'pure', 'mathematics', 'â€™', 'bristoluniarchive', 'stem', 'bristolunimaths', 'thewisecampaign']\n",
      "['join', 'with', 'me', 'in', 'celebrating', 'the', 'massive', 'contribution', 'of', 'womeninscience', 'by', 'putting', 'your', 'face', 'on', 'the', '1ofthemillion', 'women', 'in', 'stem', 'photo', 'wall']\n",
      "['proud', 'to', 'be', '1ofthemillion', 'women', 'in', 'stem', 'two', 'hearts']\n",
      "Retrieved 10 tweets about #INwx\n",
      "Current num of newhashtags: 19\n",
      "['5:00', 'am', '58', 'Â°', 'f', '(', 'h', '66', 'Â°', 'f', '/', 'l', '58', 'Â°', 'f', ')', 'clear', 'hum', ':8', '6', '%', 'wind', ':', 'n', '0', '->', '2mph', 'baro', ':', '29.88', 'in', 'rain', ':', '0.00', 'in', 'inwx']\n",
      "['latest', 'national', 'weather', 'service', 'radar', 'loop', '...', 'inwx']\n",
      "['another', 'warm', 'day', 'with', 'highs', 'in', 'the', 'mid', '80s', 'and', 'mostly', 'sunny', 'skies', '.', 'we', 'could', 'see', '90', 'Â°', 'by', 'friday', '!', 'wish_tv', 'inwx', 'indy']\n",
      "['it', 'might', 'seem', 'late', 'to', 'have', 'not', 'hit', '90', 'degrees', 'in', 'indy', 'yet', 'but', 'we', 'are', 'still', 'in', 'good', 'shape', '.', 'last', 'year', 'we', \"didn't\", 'see', '90', 'until', 'june', '29', '.', \"we've\", 'only', 'had', 'one', 'year', 'without', 'a', '90', 'degree', 'day', ',', 'back', 'in', '2004', '.', 'we', 'see', 'an', 'avg', '.', 'of', '4', 'in', 'june', '.', 'inwx', '13weather']\n",
      "['rain', 'chances', 'and', 'summer', 'heat', 'return', '.', 'best', 'chance', 'for', 'showers', '/', 'storms', 'through', 'fri', 'east-central', 'ky', '.', 'temps', 'touch', '90', 'again', 'for', 'some', '.', 'kywx', 'inwx']\n",
      "['4:00', 'am', '59', 'Â°', 'f', '(', 'h', '66', 'Â°', 'f', '/', 'l', '59', 'Â°', 'f', ')', 'clear', 'hum', ':8', '4', '%', 'wind', ':', 'n', '0', '->', '2mph', 'baro', ':', '29.88', 'in', 'rain', ':', '0.00', 'in', 'inwx']\n",
      "['latest', 'national', 'weather', 'service', 'radar', 'loop', '...', 'inwx']\n",
      "['3:00', 'am', '60', 'Â°', 'f', '(', 'h', '66', 'Â°', 'f', '/', 'l', '60', 'Â°', 'f', ')', 'clear', 'hum', ':', '78', '%', 'wind', ':', 'n', '0', '->', '2mph', 'baro', ':', '29.88', 'in', 'rain', ':', '0.00', 'in', 'inwx']\n",
      "['latest', 'national', 'weather', 'service', 'radar', 'loop', '...', 'inwx']\n",
      "['2:00', 'am', '61', 'Â°', 'f', '(', 'h', '66', 'Â°', 'f', '/', 'l', '61', 'Â°', 'f', ')', 'clear', 'hum', ':', '73', '%', 'wind', ':', 'n', '0', '->', '2mph', 'baro', ':', '29.88', 'in', 'rain', ':', '0.00', 'in', 'inwx']\n",
      "Retrieved 9 tweets about #newx\n",
      "Current num of newhashtags: 24\n",
      "['warm', '/', 'breezy', 'east', 'of', 'the', 'laramie', 'range', 'as', 'a', 'cold', 'front', 'moves', 'southeast', '.', 'cooler', 'temps', 'and', 'rain', 'are', 'in', 'the', 'extended', 'forecast', '.', 'wywx', 'newx']\n",
      "['sunny', ',', 'hot', ',', 'dry', 'and', 'breezy', 'today', '.', 'highs', ':', '95-99', '.', 'south', 'winds', 'gusting', 'to', '40', 'mph', '.', 'rain', '/', 'storm', 'chances', 'return', 'tonight', '.', '70s', 'friday', '.', 'kswx', 'newx']\n",
      "['up', 'late', '?', \"here's\", 'a', 'check', 'of', 'the', '2', 'am', 'temps', '.', 'newx']\n",
      "['very', 'dry', 'air', 'allowed', 'temperatures', 'to', 'really', 'soar', 'today', 'in', 'the', 'panhandle', '.', 'records', 'fell', 'in', 'scottsbluff', 'and', 'sidney', '.', 'newx']\n",
      "['nws', 'alert', '>', 'scattered', 'thunderstorms', 'over', 'the', 'w', 'central', 'nebraska', 'panhandle', 'moving', 'ne', 'at', '35', 'mph', '.', 'wind', 'gusts', '50', 'to', '55', 'mph', 'possible', 'with', 'these', 'storms', '.', 'strong', 'winds', 'may', 'be', 'observed', 'without', 'any', 'rain', 'or', 'lightning', '.', 'locat', '...', 'newx']\n",
      "['scattered', 'thunderstorms', 'over', 'the', 'w', 'central', 'neb', 'panhandle', 'moving', 'ne', 'at', '35', 'mph', '.', 'wind', 'gusts', '50', 'to', '55', 'mph', 'possible', 'with', 'these', 'storms', '.', 'strong', 'winds', 'may', 'be', 'observed', 'without', 'any', 'rain', 'or', 'lightning', '.', 'locations', 'impacted', '...', 'newx']\n",
      "['nws', 'alert', '>', 'scattered', 'thunderstorms', 'over', 'the', 'w', 'central', 'nebraska', 'panhandle', 'moving', 'ne', 'at', '35', 'mph', '.', 'wind', 'gusts', '50', 'to', '55', 'mph', 'possible', 'with', 'these', 'storms', '.', 'strong', 'winds', 'may', 'be', 'observed', 'without', 'any', 'rain', 'or', 'lightning', '.', 'locat', '...', 'newx']\n",
      "['mostly', 'clear', 'skies', 'tonight', 'as', 'the', 'high', 'pressure', 'slips', 'off', 'to', 'the', 'east', '.', 'keep', 'an', 'eye', 'to', 'the', 'weather', 'to', 'the', 'west', 'which', 'will', 'begin', 'to', 'approach', 'eastern', 'nebraska', 'tomorrow', 'night', 'into', 'thursday', 'newx']\n",
      "['two', 'more', 'days', 'of', '90s', 'before', \"we're\", 'back', 'into', 'the', '80s', 'for', 'a', 'bit', '.', 'the', '\"', 'cool', 'down', '\"', 'comes', 'with', 'a', 'few', 'showers', 'and', 'storms', 'on', 'and', 'off', 'thursday', 'through', 'the', 'weekend', '.', 'newx', 'iawx', '3newsnowomaha']\n",
      "Retrieved 9 tweets about #ChampionshipLeague\n",
      "Current num of newhashtags: 53\n",
      "['top', 'brazilian', 'players', 'football', 'primeraiberdrola', 'bundesliga', 'laligasantander', 'championshipleague', 'manchesterunited', 'premierleague', 'brazil', 'liverpool', 'manchestercity', 'arsenal', 'englishnews', 'covid', '_19', 'coronaviruskenya']\n",
      "[\"here's\", 'one', ';', 'what', 'did', 'the', 'championship', 'look', 'like', 'last', 'year', 'with', '9', 'games', 'to', 'go', '?', 'lufc', 'efl', 'championshipleague']\n",
      "['very', 'reliable', 'player', '.', 'good', 'choice', 'as', 'a', 'replacement', 'for', 'hakimi', '.', 'an', 'experienced', 'guy', 'with', 'very', 'good', 'shot', 'and', 'technique', 'who', 'can', 'fit', 'perfectly', 'in', 'dortmund', 'attack', 'plan', '!', 'championshipleague', 'bundesliga', 'ligue', '1conforama', 'psg_english', 'blackyellow']\n",
      "['one', 'more', 'chance', 'lfc', 'istanbul', 'final', 'move', '2021', '!', '!', '!', 'let', 'â€™', 's', 'make', 'third', 'trophy', 'trophy', 'trophy', 'liverpool', 'championshipleague']\n",
      "[\"haven't\", 'had', 'this', 'much', 'fun', 'since', 'last', 'seasons', 'championshipleague', 'semi', 'finals', 'serious', 'face', 'with', 'symbols', 'covering', 'mouth', 'serious', 'face', 'with', 'symbols', 'covering', 'mouth', 'overheated', 'face', 'overheated', 'face']\n",
      "['cpfc', ',', \"i'm\", 'ready', 'to', 'be', 'back', ',', 'louder', 'than', 'ever', 'screaming', 'at', 'the', 'tv', '.', 'heavy', 'black', 'heart', 'ï¸', 'blue', 'heart', 'soccer', 'socceraid', 'football', 'crystalpalace', 'premierleague', 'championshipleague']\n",
      "['lc210475', 'robbo', 'sure', 'did', 'like', 'the', 'vision', 'of', 'himself', 'questioning', 'the', 'lads', 'rowell', '&', 'anderson', 'from', 'last', 'year', '!', 'headwobble', 'robbocop', 'championshipleague', 'ronjeremymedal', '3votes']\n",
      "['leedsunited', 'are', 'less', 'than', 'ten', 'wins', 'away', 'from', 'ending', 'their', '16', '-', 'year', 'wait', 'for', 'a', 'premierleague', 'return', '!', 'epl', 'championshipleague']\n",
      "['i', 'wished', 'i', 'had', 'a', 'quarter', 'of', 'the', 'confidence', 'this', 'kid', 'has', 'face', 'with', 'tears', 'of', 'joy', 'laliga', 'championshipleague']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 6 tweets about #hokies\n",
      "Current num of newhashtags: 58\n",
      "['colleges', 'links', 'to', 'virginia', 'tech', 'athletics', 'virginia', 'tech', 'hokies', 'athletics']\n",
      "['bypatforde', 'anybody', 'done', 'fuente', 'yet', '?', 'or', 'bud', '?', 'or', 'frank', '?', 'hokies']\n",
      "['props', 'to', 'cartsquotes', 'for', 'doing', 'a', 'lot', 'of', 'the', 'heavy', 'lifting', 'on', 'this', 'project', 'that', 'we', 'â€™', 'll', 'continue', 'to', 'evolve', '!', 'hokies', 'turkey']\n",
      "['another', 'hokies', 'women', 'â€™', 's', 'basketball', 'assistants', 'is', 'headed', 'elsewhere', '.', 'there', 'are', 'now', 'two', 'assistant', 'openings', 'on', 'kenny', 'brooks', 'â€™', 'staff', '.']\n",
      "['hokies', 'women', 'â€™', 's', 'basketball', 'assistant', 'coach', 'britney', 'anderson', 'is', 'headed', 'to', 'stanford', '.', 'a', 'tech', 'alumnus', 'who', 'played', 'in', 'blacksburg', 'from', '2003-07', ',', 'anderson', 'was', 'an', 'assistant', 'from', '2015-20', 'under', 'dennis', 'wolff', 'and', 'then', 'kenny', 'brooks', '.']\n",
      "['why', 'virginia', 'tech', '?', 'transfer', '\\u2066', 'jm_squarepants', '\\u2069', 'explains', 'â€œ', 'i', 'wanted', 'to', 'be', 'able', 'to', 'put', 'myself', 'in', 'the', 'best', 'position', 'to', 'be', 'successful', 'â€', '(', 'vip', ')', 'hokies']\n",
      "Retrieved 10 tweets about #gotiges\n",
      "Current num of newhashtags: 64\n",
      "['no', 'dusty', '!', 'crying', 'face', 'gotiges']\n",
      "['dustin', 'is', 'obviously', 'a', 'huge', 'omission', ',', 'but', 'i', 'can', 'think', 'of', 'plenty', 'worse', 'ins', 'than', 'josh', 'caddy', '.', 'still', 'in', 'good', 'sorts', 'tiger', 'face', 'gotiges']\n",
      "['no', 'dusty', 'loudly', 'crying', 'face', 'loudly', 'crying', 'face', 'afltigershawks', 'gotiges']\n",
      "['we', 'shall', 'pulp', 'hawthorn', 'while', 'dusty', 'puts', 'his', 'feet', 'up', '.', 'i', 'just', 'hope', 'he', \"doesn't\", 'hurt', 'himself', 'chortling', '.', 'gotiges']\n",
      "['no', 'dusty', 'this', 'week', '.', 'at', 'least', 'we', 'got', 'caddy', 'back', 'person', 'raising', 'both', 'hands', 'in', 'celebration', 'gotiges']\n",
      "['flash', 'gotiges', 'changes', 'for', 'afltigershawks', 'afl', 'match', 'in', ':', 'jack', 'ross', ',', 'josh', 'caddy', 'out', ':', 'dustin', 'martin', '(', 'ribs', ')', ',', 'liam', 'baker', '(', 'personal', ')']\n",
      "['we', 'went', 'to', 'port', 'last', 'yr', 'no', 'rance', 'cotchin', 'jack', 'or', 'dusty', '&', 'won', '.', \"we'll\", 'be', 'fine', 'gotiges']\n",
      "['f', '..', 'ck', '!', '!', '!', 'no', 'dusty', 'gotiges']\n",
      "['any', 'truth', 'is', 'dusty', 'being', 'ruled', 'out', '?', 'gotiges']\n",
      "['dusty', 'out', 'of', 'tomorrow', 'night', 'â€™', 's', 'clash', '...', 'worried', 'face', 'afl', 'gotiges']\n",
      "Retrieved 9 tweets about #TWUG\n",
      "Current num of newhashtags: 75\n",
      "['brendamoen1', 'donniewahlberg', 'sending', 'a', 'huge', 'twug', 'to', 'donniewahlberg', 'and', 'all', 'the', 'bh', 'family', 'heavy', 'black', 'heart', 'ï¸', 'permanent', 'paper', 'sign', 'ï¸', 'robot', 'face', 'heavy', 'black', 'heart', 'ï¸']\n",
      "['donniewahlberg', 'twug', 'buddy']\n",
      "['juskeepbreathn', 'bounlay2311', 'cnoelddub4life', 'karastraitup', 'xfadingangel', 'donniemaggie', 'andreaknightb', 'ddubwifey89', 'maevabluebloods', 'caroclou', 'angelaw20427503', 'jaimedonniegirl', 'ac2cnkotb', 'tammywalker2334', 'brunel_nadege', 'ruth_a_day', 'jmfnkotb1', 'sherriefinch16a', '4cherbear67', 'dolphinmg', 'irishjones613', 'petrinajoyce', 'aprilwagner19', 'ddubgirl69', 'detroit_gem08', 'junebug61767', 'jennyraej1979', 'erikankotb', 'asure76', 'newnkotbgal', 'cassie040683', 'tangnikaia', 'michigan_yaller', 'jordanyascomich', 'naturenohurry91', 'jojo_brown50', 'susanalicious', 'noelbhddub78', 'angelaw44038090', 'nkotbblockhead1', 'maria_marta', 'lisau123', 'donniewahlberg', 'shergiggles', 'mri3ofus', 'angwar72', 'toughnkotbfan', 'nkotb_i', 'tweet_me_ddub', 'fabulous_kimmy', 'twug', 'twug', 'tuesday', 'bhlove', 'love', '2all', 'lovetheshitoutofyou', 'loveeternal', 'robot', 'face', 'heavy', 'black', 'heart', 'ï¸', 'permanent', 'paper', 'sign', 'ï¸']\n",
      "['krystalchonko', 'donniewahlberg', 'happy', 'birthday', '!', 'today', '(', 'june', '16', ')', 'is', 'my', '43rd', 'bday', '!', 'twug']\n",
      "['donniewahlberg', 'twug', 'thanks', '.']\n",
      "['ruth_a_day', \"i'm\", 'hanging', 'tough', 'ruth', 'hugging', 'face', 'heavy', 'black', 'heart', 'twug']\n",
      "['jonsprincess85', 'asure76', 'prshegrl718', 'catlett_trisha', 'nkotb', 'babyqueennala74', 'jennyraej1979', 'ac2cnkotb', 'brendamoen1', 'karastraitup', 'love', 'twug', 'tuesday']\n",
      "['juskeepbreathn', 'bounlay2311', 'catlett_trisha', 'karastraitup', 'xfadingangel', 'donniemaggie', 'andreaknightb', 'ddubwifey89', 'maevabluebloods', 'caroclou', 'angelaw20427503', 'jaimedonniegirl', 'ac2cnkotb', 'tammywalker2334', 'brunel_nadege', 'ruth_a_day', 'jmfnkotb1', 'sherriefinch16a', '4cherbear67', 'dolphinmg', 'irishjones613', 'petrinajoyce', 'aprilwagner19', 'ddubgirl69', 'detroit_gem08', 'junebug61767', 'jennyraej1979', 'erikankotb', 'asure76', 'newnkotbgal', 'cassie040683', 'tangnikaia', 'michigan_yaller', 'jordanyascomich', 'naturenohurry91', 'jojo_brown50', 'susanalicious', 'noelbhddub78', 'angelaw44038090', 'nkotbblockhead1', 'maria_marta', 'lisau123', 'donniewahlberg', 'shergiggles', 'mri3ofus', 'angwar72', 'toughnkotbfan', 'nkotb_i', 'tweet_me_ddub', 'fabulous_kimmy', 'always', 'and', 'forever', 'my', 'bh', 'family', \"i'm\", 'so', 'thankful', 'for', 'you', 'all', 'twug']\n",
      "['bounlay2311', 'cnoelddub4life', 'catlett_trisha', 'karastraitup', 'xfadingangel', 'donniemaggie', 'andreaknightb', 'ddubwifey89', 'maevabluebloods', 'caroclou', 'angelaw20427503', 'jaimedonniegirl', 'ac2cnkotb', 'tammywalker2334', 'brunel_nadege', 'ruth_a_day', 'jmfnkotb1', 'sherriefinch16a', '4cherbear67', 'dolphinmg', 'irishjones613', 'petrinajoyce', 'aprilwagner19', 'ddubgirl69', 'detroit_gem08', 'junebug61767', 'jennyraej1979', 'erikankotb', 'asure76', 'newnkotbgal', 'cassie040683', 'tangnikaia', 'michigan_yaller', 'jordanyascomich', 'naturenohurry91', 'jojo_brown50', 'susanalicious', 'noelbhddub78', 'angelaw44038090', 'nkotbblockhead1', 'maria_marta', 'lisau123', 'donniewahlberg', 'shergiggles', 'mri3ofus', 'angwar72', 'toughnkotbfan', 'nkotb_i', 'tweet_me_ddub', 'fabulous_kimmy', 'twug', 'twug', 'tuesday', 'hugging', 'face', 'lovetheshitouttayou', 'love', '2all', 'bhlove', 'loveeternal', 'robot', 'face', 'heavy', 'black', 'heart', 'ï¸', 'permanent', 'paper', 'sign']\n",
      "Retrieved 10 tweets about #filthyrichhomeless\n",
      "Current num of newhashtags: 82\n",
      "['arronwood', 'i', 'â€™', 'm', 'a', 'bit', 'behind', ',', 'i', 'know', ',', 'but', 'i', 'watched', 'the', 'first', 'two', 'episodes', 'of', 'filthyrichhomeless', 'last', 'night', '.', 'what', 'an', 'emotional', 'roller', 'coaster', '!', 'it', 'makes', 'my', 'heart', 'heavy', 'to', 'know', 'there', 'are', '8000', 'rough', 'sleepers', 'out', 'there', '.', 'i', 'â€™', 'll', 'be', 'adding', 'to', 'your', 'total', 'shortly', '.']\n",
      "['my', 'salvos', 'salvos614', 'brendannottle', 'fundraiser', 'is', '$', '12,283', '!', 'amazing', 'generosity', 'from', 'so', 'many', 'during', 'tough', 'times', 'for', 'our', 'most', 'vulnerable', '.', 'i', 'â€™', 'm', 'worried', 'i', 'might', 'fall', 'short', 'of', 'the', '$', '20,000', 'target', ',', 'but', 'a', 'huge', 'thanks', 'to', 'all', 'who', 'â€™', 've', 'donated', 'person', 'with', 'folded', 'hands', 'filthyrichhomeless']\n",
      "['jiggsy', 'good', 'pull-up', 'that', 'it', 'would', 'filthyrichhomeless', 'spaceship', 'is', 'an', 'oxymoron', '-', 'more', 'a', 'grosslackofspaceship']\n",
      "['watch', 'filthyrichhomeless', 'here', ':']\n",
      "['a', 'massive', 'thank', 'you', 'to', 'elliegonsalves', 'for', 'joining', 'us', 'on', 'hybpa', '!', 'you', 'can', 'watch', 'ellie', 'on', 'filthyrichhomeless', 'now', 'on', 'sbsondemand', '.']\n",
      "['everyone', 'who', 'watched', 'filthyrichhomeless', ',', 'we', 'â€™', 're', 'currently', 'searching', 'for', 'affordable', 'housing', 'for', 'eden', '.', 'let', 'me', 'know', 'if', 'there', 'are', 'any', 'suggestions', 'or', 'someone', 'you', 'can', 'put', 'us', 'on', 'to', '!', 'our', 'go', 'fund', 'me', 'for', 'eden', 'just', 'reached', 'over', '$', '7,000', 'clapping', 'hands', 'sign', 'emoji', 'modifier', 'fitzpatrick', 'type', '-', '3', 'sparkles']\n",
      "['arronwood', 'salvos', 'well', 'done', '&', 'thank', 'you', '.', 'can', 'you', 'issue', 'a', 'challenge', 'to', 'your', 'interstate', 'counterparts', 'to', 'match', 'what', \"you're\", 'doing', '&', 'to', 'participate', 'next', 'year', 'in', 'filthyrichhomeless', '?', 'there', 'is', 'nothing', 'like', 'walking', 'a', 'day', 'in', 'another', \"person's\", 'shoes', '.', 'me', 'thinks', 'community', 'service', 'should', 'be', 'compulsory', 'for', 'all', '.']\n",
      "['we', \"can't\", 'wait', 'to', 'welcome', 'model', '&', 'actress', 'elliegonsalves', 'from', 'filthyrichhomeless', 'as', 'guest', 'quizmaster', '!', 'new', 'hybpa', ',', '8.40', 'tonight', 'on', 'channel10au', '!']\n",
      "['sbs', '_andrewrochford', 'filthyrichhomeless', 'the', 'current', 'increase', 'in', 'the', 'job', 'seeker', 'allowance', 'can', 'help', 'the', 'homeless', 'find', 'a', 'room', ',', 'get', 'a', 'life', ',', 'get', 'a', 'job', '.', 'every', 'effort', 'should', 'be', 'made', 'to', 'continue', 'this', 'payment', '.', 'tks', 'for', 'what', 'you', 'do', 'two', 'hearts', 'homeless', 'effort', 'homelessness']\n",
      "['that', 'is', 'sadly', 'not', 'unusual', '.', 'filthyrichhomeless', 'everybodyshome']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 10 tweets about #RikMayall\n",
      "Current num of newhashtags: 104\n",
      "['â€œ', 'bang', 'on', '!', 'â€', 'rikmayall']\n",
      "['at', 'least', 'tory', 'mps', 'never', 'remotely', 'come', 'come', 'across', 'as', 'comedic', 'swivel-eyed', 'frothing-at-the-mouth', 'fascists', '.', 'even', 'rik', 'mayall', 'didn', 'â€™', 't', 'do', 'them', 'this', 'well', '.', 'who', 'votes', 'for', 'this', '?', 'savedfid', 'dfid', 'overseasaid', 'aid', 'humanityfirst', 'rikmayall', 'alanbstard', 'torybritain']\n",
      "['they', 'say', 'romance', 'is', 'dead', '?', 'rikmayall', 'blackadder']\n",
      "['sholamos1', 'get', 'used', 'to', 'it', '.', 'you', 'all', 'belong', 'to', 'us', 'now', 'smiling', 'face', 'with', 'horns', 'torybritain', 'rikmayall', 'alanbstard']\n",
      "['how', 'we', 'are', 'all', 'feeling', 'now', 'lockdownextended', 'rikmayall', 'bottom']\n",
      "['2', 'new', 'vintage', 'styled', 'bad', 'news', 'designs', 'now', 'available', 'at', 'and', 'badnews', 'rikmayall', 'talkingbottom', 'remembering_rik', 'rikscrapbook', 'mayallfan', 'dreamytimee', 'rikmayallquotes', 'ade_rik', 'memorialrik', 'mayallonline']\n",
      "['night', 'night', 'bottom', 'fans', '.', 'see', 'you', 'tomorrow', '(', '\"', 'not', 'too', 'early', '!', '\"', ')', 'for', 'more', 'fun', 'and', 'games', 'thumbs', 'up', 'sign', 'emoji', 'modifier', 'fitzpatrick', 'type', '-1-2', 'bottom', 'rikmayall', 'adeedmondson', 'richieandeddie', 'richardrichard', 'edwardhitler', 'hammersmith', 'wearemenofscience']\n",
      "['sarah', 'furgeson', '.', 'the', 'shame', '!', 'rikmayall', 'bottom']\n",
      "['what', 'is', 'the', 'phase', '?', 'wrong', 'answers', 'only', 'blackadder', 'rikmayall', 'comedy']\n",
      "['as', 'a', 'huge', 'rik', 'mayall', 'fan', '-', 'this', 'is', 'the', 'best', 'thing', \"i've\", 'seen', 'today', '...', 'happy', 'tuesday', 'everyone', 'rikmayall']\n",
      "Retrieved 10 tweets about #BBAU\n",
      "Current num of newhashtags: 108\n",
      "['so', 'uhh', 'i', 'â€™', 'm', 'all', 'caught', 'up', 'and', 'what', 'the', 'fuck', 'ian', 'bbau']\n",
      "['in', 'case', 'you', 'missed', 'it', 'earlier', ':', 'xavier', 'takes', 'a', 'jab', 'at', 'big', 'brother', 'producers', 'for', 'only', 'airing', 'the', '\"', 'b--ching', 'and', 's--t', '\"', 'on', 'the', 'main', 'show', '.', 'bbau']\n",
      "['no', 'way', 'kieran', 'is', 'that', 'bad', 'at', 'driving', 'bbau']\n",
      "['catching', 'up', 'on', 'that', 'bbau']\n",
      "['crying', 'and', 'popping', 'are', 'the', 'best', 'things', 'ever', '.', \"it's\", 'like', 'let', 'it', 'go', ',', 'let', 'it', 'go', 'angela', 'bbau']\n",
      "['â€˜', 'they', 'all', 'need', 'to', 'watch', 'out', '!', 'â€™', ':', 'angela', 'spills', 'the', 'tea', 'on', 'the', 'big', 'brother', 'housemates', 'bbau']\n",
      "['majorglitch98', 'happens', 'every', 'year', 'with', 'all', 'the', 'bb', 'around', 'the', 'globe', '.', 'its', 'disgusting', 'af', '.', 'in', 'bb15', 'the', 'fans', 'did', 'actually', 'get', 'people', 'fired', ',', 'some', 'of', 'them', 'were', 'shady', 'af', 'but', 'nobody', 'deserves', 'abuse', 'on', 'a', 'fun', 'reality', 'show', '.', 'they', 'all', 'should', 'be', 'grateful', 'bbau', 'is', 'back', '.', 'i', 'know', 'i', 'sure', 'am']\n",
      "['i', 'tweet', 'a', 'lot', 'of', 'bbau', 'it', 'â€™', 's', 'my', 'fave', 'show', 'of', 'all', 'time', 'so', 'if', 'u', 'don', 'â€™', 't', 'like', 'it', 'mute', 'the', 'hashtag', '.', 'thanks', 'person', 'with', 'folded', 'hands']\n",
      "['this', 'is', 'it', '.', 'this', 'is', 'the', 'best', 'thing', 'i', 'â€™', 've', 'read', 'in', 'my', 'entire', 'life', '.', 'thank', 'you', 'queen', 'angela', '.', 'bbau']\n",
      "['bbaureturns', 'robcesternino', 'if', \"there's\", 'time', 'can', 'we', 'get', 'armstrongtaran', 'that', 'big', 'brother', 'takeover', 'clip', 'because', 'bbau', \"might've\", 'actually', 'made', 'it', 'exciting', '.']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import itertools\n",
    "\n",
    "newhashtags = []\n",
    "X_document = []\n",
    "y_document = []\n",
    "\n",
    "print(len(available_hashtags))\n",
    "tweettokenizer = TweetTokenizer()\n",
    "\n",
    "charsequence = [\".\",\"#\",\":\",\"â€¢\",\",\",\"@\",\"\\\"\",\";\",\"\\'\",\")\",\"(\",\"&\",\"``\",\"\\'\\'\",\"-\",\"Â°\",\"/\",\"...\",\"rt\",\">\",\"<\",\"%\",\"\"]\n",
    "pipe = Pipeline(\n",
    "        [(\"tokenizetweet\",Tokenizer(tweettokenizer)),\n",
    "         (\"rmemoji\",ConvertEmojis(tweettokenizer)),\n",
    "         (\"standardizetweets\",Standardize()),\n",
    "        ]\n",
    ")\n",
    "\n",
    "for i in range(len(available_hashtags.iloc[:10,0])):\n",
    "    hashtag = available_hashtags.iloc[i,0]\n",
    "    \n",
    "    extractor = HashtagDetailsExtractor(10)\n",
    "    X,y = extractor.extract(hashtag)\n",
    "    X_decoded = extractor.decode()\n",
    "    \n",
    "    print(\"Retrieved \" + str(len(X_decoded)) + \" tweets about \" +  hashtag)\n",
    "    \n",
    "    [newhashtags.extend(newhashtag) for newhashtag in y]\n",
    "    newhashtags = list(set(newhashtags))\n",
    "    print(\"Current num of newhashtags: \" + str(len(newhashtags)))\n",
    "    \n",
    "    hashtag_X_doc = pipe.fit_transform(X_decoded)\n",
    "    \n",
    "    \n",
    "    with open(\"mined_data/X_document.csv\", \"a\") as fp:\n",
    "        wr = csv.writer(fp)\n",
    "        for entry in hashtag_X_doc:\n",
    "            print(entry)\n",
    "            wr.writerow(entry)\n",
    "        \n",
    "    with open(\"mined_data/y_document.csv\", \"a\") as fp:\n",
    "        wr = csv.writer(fp)\n",
    "        for entry in y:\n",
    "            wr.writerow(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T08:09:12.741548Z",
     "start_time": "2020-06-17T08:09:07.331Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"mined_data/X_document_200611.csv\",\"w+\") as X_document_csv:\n",
    "    csvWriter = csv.writer(X_document_csv,delimiter=',')\n",
    "    csvWriter.writerows(X_document)\n",
    "    \n",
    "with open(\"mined_data/y_document_200611.csv\",\"w+\") as y_document_csv:\n",
    "    csvWriter = csv.writer(y_document_csv,delimiter=',')\n",
    "    csvWriter.writerows(y_document)\n",
    "    \n",
    "with open(\"mined_data/newhashtags_200611.csv\",\"w+\") as newhashtags_csv:\n",
    "    csvWriter = csv.writer(newhashtags_csv,delimiter=',')\n",
    "    csvWriter.writerows(newhashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T08:57:26.936693Z",
     "start_time": "2020-06-17T08:57:26.924416Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import html\n",
    "\n",
    "class HashtagDetailsExtractor:\n",
    "    \n",
    "    def __init__(self,tweetnum=1000):\n",
    "        self.tweetnum = tweetnum\n",
    "        self.tweet_text_list = []\n",
    "        self.hashtaglists = []\n",
    "        \n",
    "    def decode(self):\n",
    "        decodedtweets = []\n",
    "        for tweet in self.tweet_text_list:\n",
    "            decodedtweets.append(html.unescape(tweet.decode(\"utf8\").replace(\"#\",\"\")))\n",
    "            \n",
    "        return decodedtweets\n",
    "        \n",
    "        \n",
    "    def extract(self,tag):\n",
    "        for tweet in tweepy.Cursor(api.search,q=tag + \" -filter:retweets\",tweet_mode=\"extended\",full_text=True).items(self.tweetnum):\n",
    "            if(tweet.lang==\"en\"):\n",
    "#                 tweet_text_list.append(tweet._json[\"full_text\"].lower())\n",
    "#                 if(not ('retweeted_status' in tweet._json)):\n",
    "                self.tweet_text_list.append(tweet.full_text.encode(\"utf8\"))\n",
    "                hashtaglist = re.findall( r'#\\w\\w*',self.tweet_text_list[-1].decode(\"utf8\"))\n",
    "                self.hashtaglists.append(list(set(hashtaglist)))\n",
    "                    \n",
    "#                     print(tweet.full_text.encode(\"utf8\"))\n",
    "#                     self.tweet_text_list.append(tweet._json['retweeted_status']['full_text'].encode(\"utf8\"))\n",
    "#                 else:\n",
    "#                     self.tweet_text_list.append(tweet.full_text.encode(\"utf8\"))\n",
    "                \n",
    "                \n",
    "#                 hashtags = tweet._json[\"entities\"][\"hashtags\"]\n",
    "#                 hashtaglist = []\n",
    "#                 hashtaglist.append(tag)\n",
    "#                 for i in range(len(hashtags)):\n",
    "#                     hashtagstring = \"#\" + hashtags[i][\"text\"]\n",
    "#                     hashtaglist.append(hashtagstring.lower())\n",
    "                \n",
    "                \n",
    "        \n",
    "        return self.tweet_text_list, self.hashtaglists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:30:13.988243Z",
     "start_time": "2020-06-16T14:30:11.995603Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor = HashtagDetailsExtractor(10)\n",
    "\n",
    "X,y = extractor.extract(\"#love\")\n",
    "\n",
    "X_decoded = extractor.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:30:16.603102Z",
     "start_time": "2020-06-16T14:30:16.594234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch @gregrharvey's broadcast: YOUR Special Purpose / Choices / inspire love UnitedNotDivided qanon / https://t.co/yRzWvGB9S2 https://t.co/EP5wAX8854\n",
      "=======\n",
      "\n",
      "Love is our most powerful resource. Itâ€™s the boundless energy that connects all matter and draws like to like.  TuesdayThoughts LoveSchool TheLoveDiet\n",
      "=======\n",
      "\n",
      "Live your life more in the present moments. Be aware, be happy, trust, and love yourself first, and then allow life to happen. blessings grateful peace love trust spirituality psychic believeâ€¦ https://t.co/3jQxpf8o1O\n",
      "=======\n",
      "\n",
      "Hues of sunshine to brighten up your Tuesday mood! ðŸ˜\n",
      "\n",
      "tuesdayvibes Love couplegoals\n",
      "WeddingWire Mehndi https://t.co/hjfTwX2jrA\n",
      "=======\n",
      "\n",
      "FASHION TIPS & TRICKS TO CHANGE YOUR LIFE\n",
      "\n",
      "TuesdayMotivation: Fill the closet w/ simple summer chic outfits Â Â Â https://t.co/F5d72YiKXL\n",
      "\n",
      "fashion girl love beauty inspiration shopping workout cute hot motivation makeup art sexy style TipTuesday model etsy love https://t.co/3DvSbrOsOM\n",
      "=======\n",
      "\n",
      "Join Essence Of Life https://t.co/R2SNnoFUxV ðŸ˜‡ðŸŒ¹ðŸ’•  Allah advice beautiful belief blessings care change community empathy faith friends group inspire  faith like4like likeforlike love optimism peace positive quote relationship smile support thought. https://t.co/4wqMxAbKvp\n",
      "=======\n",
      "\n",
      "â¤ï¸ donâ€™t ever give up hope, you are stronger than you think    â¤ï¸ ðŸ˜€ \n",
      "\n",
      "positive affirmation positivity possibilities positivethinking happy motivation dreams goals believe happiness relaxation love solution kind gratitude kindness grateful  earlybiz â¤ï¸ https://t.co/fZ2oKBCkEj\n",
      "=======\n",
      "\n",
      "My happy place â¤ï¸ family love calebvincenthoyt hubby https://t.co/w4veuFR2Gj\n",
      "=======\n",
      "\n",
      "left like the waves\n",
      "to death alone\n",
      "they would \n",
      "in velveteen legs\n",
      "of the sea\n",
      "be on the stillness\n",
      "of fatherâ€™s ankle;\n",
      "a withering \n",
      "of loneliness\n",
      "I mourn in the tree\n",
      "I fell.\n",
      "\n",
      "Poetry Writing vss MadVerse Love WritersCafe Literature Grief\n",
      "=======\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(X_element + \"\\n=======\\n\") for X_element in X_decoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:30:20.998731Z",
     "start_time": "2020-06-16T14:30:20.991633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#UnitedNotDivided', '#inspire', '#qanon', '#love']\n",
      "['#TheLoveDiet', '#TuesdayThoughts', '#Love', '#LoveSchool']\n",
      "['#grateful', '#believe', '#psychic', '#blessings', '#love', '#spirituality', '#trust', '#peace']\n",
      "['#couplegoals', '#Tuesday', '#Mehndi', '#tuesdayvibes', '#WeddingWire', '#Love']\n",
      "['#motivation', '#model', '#shopping', '#LIFE', '#beauty', '#girl', '#summer', '#hot', '#inspiration', '#love', '#TuesdayMotivation', '#sexy', '#style', '#TipTuesday', '#art', '#fashion', '#workout', '#makeup', '#TIPS', '#cute', '#etsy']\n",
      "['#community', '#inspire', '#smile', '#friends', '#thought', '#beautiful', '#love', '#Life', '#quote', '#optimism', '#empathy', '#peace', '#advice', '#change', '#likeforlike', '#support', '#relationship', '#faith', '#group', '#belief', '#blessings', '#care', '#Allah', '#positive', '#like4like']\n",
      "['#grateful', '#happy', '#motivation', '#possibilities', '#positivethinking', '#goals', '#relaxation', '#love', '#solution', '#dreams', '#kind', '#positivity', '#earlybiz', '#affirmation', '#believe', '#happiness', '#gratitude', '#positive', '#kindness']\n",
      "['#family', '#calebvincenthoyt', '#hubby', '#love']\n",
      "['#Poetry', '#vss', '#MadVerse', '#WritersCafe', '#Writing', '#Literature', '#Grief', '#Love']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(y_element) for y_element in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:51.402458Z",
     "start_time": "2020-06-16T13:36:50.242715Z"
    }
   },
   "outputs": [],
   "source": [
    "from  nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns tokenized text with the supplied Tokenizer \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,tokenizer_model,lowercase=True):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        tokenized_tweets = []\n",
    "        \n",
    "        for tweet_text in X:\n",
    "            tweet_text_tokenized = self.tokenizer_model.tokenize(tweet_text)\n",
    "            tokenized_tweets.append(tweet_text_tokenized)\n",
    "            \n",
    "        return tokenized_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T13:34:32.661075Z",
     "start_time": "2020-06-10T13:34:32.585542Z"
    }
   },
   "outputs": [],
   "source": [
    "tweettokenizer = TweetTokenizer()\n",
    "\n",
    "tokenizer = Tokenizer(tweettokenizer)\n",
    "\n",
    "X_tokenized = tokenizer.fit_transform(X_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T13:36:52.186057Z",
     "start_time": "2020-06-16T13:36:52.143713Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import copy\n",
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import unicodedata as ud\n",
    "\n",
    "class ConvertEmojis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer_model):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def hasemoji(self,s):\n",
    "        em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "        em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "        em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "        emojiExists = False\n",
    "        for emojiTest in em_split:\n",
    "            if(emojiTest in UNICODE_EMOJI):\n",
    "                emojiExists = True\n",
    "    \n",
    "        return emojiExists\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        emoji_seperator = TweetTokenizer()\n",
    "        X_copy = copy.deepcopy(X)\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "            for ii,word_token in enumerate(tweet):\n",
    "                if(self.hasemoji(word_token)):\n",
    "                    em_split_emoji = emoji.get_emoji_regexp().split(word_token)\n",
    "                    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                    emoji_detail_tokenized = []\n",
    "                    words = \"\"\n",
    "                    for iii,each_emoji in enumerate(em_split):\n",
    "                        try:\n",
    "                            emoji_detail = ud.name(each_emoji)\n",
    "                            emoji_tokenized = self.tokenizer_model.tokenize(emoji_detail.lower())\n",
    "                            if(emoji_tokenized[-1]==\"selector-16\" and emoji[-2]==\"variation\"):\n",
    "                                emoji_tokenized.clear()\n",
    "                            emoji_detail_tokenized.extend(emoji_tokenized)\n",
    "                        except:\n",
    "                            emoji_detail_tokenized.extend([each_emoji])\n",
    "                            pass\n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + emoji_detail_tokenized + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(emoji_detail_tokenized) - 1\n",
    "        return X_copy\n",
    "    \n",
    "class RemovePunctuation(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,charsequence):\n",
    "        self.charsequence = set(charsequence)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for i,tweet in enumerate(X):\n",
    "            \n",
    "            X_copy[i] = [text for text in tweet if not set([text]).issubset(self.charsequence)]\n",
    "            \n",
    "        return X_copy\n",
    "    \n",
    "class Standardize(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "#     def __init__(self,charsequence):\n",
    "#         self.charsequence = set(charsequence)\n",
    "        \n",
    "    def fit(self, X , y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        standardizedtweets = []\n",
    "        \n",
    "        for i,tweet in enumerate(X):\n",
    "            rl_tknzedtweets = []\n",
    "            \n",
    "            for ii,word in enumerate(tweet):\n",
    "\n",
    "                if(word[0] == \"#\" or word[0] == \"@\"):\n",
    "                    rl_tknzedtweets.append(word[1:].lower())\n",
    "                elif(word[:8] == \"https://\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    rl_tknzedtweets.append(word.lower())\n",
    "                    \n",
    "            standardizedtweets.append(rl_tknzedtweets)\n",
    "        \n",
    "        return standardizedtweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T13:35:58.285956Z",
     "start_time": "2020-06-10T13:35:57.708071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['all', 'i', 'can', 'be', 'music', 'riff', 'guitarist', 'art', 'artist', 'love', 'poetry', 'musician', 'lyrics', 'lyricist', 'video', 'viralvideos', 'peace', 'viral', 'freemusic', '11'], ['behold', 'the', 'mimimi', 'love', 'package', 'orange', 'heart', 'a', 'special', 'limited', 'version', 'of', 'the', 'desperados3', \"collector's\", 'edition', 'that', 'is', 'exclusively', 'available', 'in', 'our', 'eshop', 'made', 'w', '/', 'mimimi', 'love', 'very', 'cool', 'the', 'limited', 'movie-style', 'desperados3', 'poster', 'signed', 'by', 'us', '!', 'pre-order', 'now'], ['follow', 'me', 'for', 'more', 'quotes', 'on', 'love', 'relationships', 'soulmates', 'womanhood', 'positivethoughts'], ['without', 'it', 'relationships', 'can', 'get', 'ugly', '...', 'via', 'youtube', 'men', 'women', 'love', 'relationships', 'dating', 'datingsteadygopodcast'], ['wonderful', 'evening', 'and', 'good', 'night', 'to', 'everyone', 'call', 'me', 'hand', 'surfer', 'emoji', 'modifier', 'fitzpatrick', 'type', '-1-2', '\\u200d', 'male', 'sign', 'ï¸', 'surfer', '\\u200d', 'female', 'sign', 'ï¸', 'sleeping', 'face', 'sunset', 'surf', 'saltlife', 'playa', 'ocean', 'vitaminsea', '\\u2060', '\\u2060', '\\u2060', '\\u2060', 'goodvibesonly', 'love', 'enjoytheview'], ['i', 'dreamt', 'an', 'angel', 'by', 'artist', 'palette', 'copyright', 'sign', 'ï¸bruce', 'neeley', '2020', 'writing', 'hand', 'emoji', 'modifier', 'fitzpatrick', 'type', '-1-2', 'copyright', 'sign', 'ï¸tereza', 'gillespie', '2020', 'art', 'poetry', 'love', 'peace', 'heavy', 'black', 'heart', 'ï¸', 'shamrock', 'ï¸', 'front-facing', 'baby', 'chick', 'heavy', 'black', 'heart', 'ï¸', 'peace', 'symbol', 'ï¸', 'heavy', 'black', 'heart', 'ï¸', 'hugging', 'face', 'sparkles', 'person', 'with', 'folded', 'hands', 'rose', 'permanent', 'paper', 'sign', 'staysafe', 'staywell', 'stayfree', 'blm', 'equality', 'loveiseverything', 'alwaysbekind', 'heavy', 'black', 'heart', 'ï¸', 'peace', 'symbol', 'ï¸', 'person', 'with', 'folded', 'hands', 'sparkles', 'earth', 'globe', 'americas', 'earth', 'globe', 'asia-australia', 'earth', 'globe', 'europe-africa', 'sparkles', 'person', 'with', 'folded', 'hands', 'peace', 'symbol', 'ï¸', 'heavy', 'black', 'heart', 'ï¸'], ['fragile', 'barry', 'poetry', 'poet', 'writer', 'art', 'poetrycommunity', 'poetrywriter', 'scarletmonahan', 'love', 'words', '5'], ['helping', 'singles', 'meet', 'the', 'old', 'fashioned', 'way', 'singles', 'wearthelotus', 'meet', 'love', 'findlove', 'datingtips', 'dating'], ['restless', 'freemusic', 'music', 'riff', 'guitarist', 'art', 'artist', 'love', 'poetry', 'musician', 'lyrics', 'lyricist', 'video', 'viral', '7'], ['wednesdaythoughts', 'love', 'yourself', 'and', 'give', 'love', 'it', 'will', 'always', 'come', 'back', 'to', 'you', '...']]\n"
     ]
    }
   ],
   "source": [
    "perm = np.random.RandomState(seed=13).permutation(10)*10+200\n",
    "\n",
    "emojiConv = ConvertEmojis(tweettokenizer)\n",
    "X_no_emoji = emojiConv.fit_transform(X_tokenized)\n",
    "\n",
    "\n",
    "charsequence = [\".\",\"#\",\":\",\"â€¢\",\",\",\"@\",\"\\\"\",\";\",\"\\'\",\")\",\"(\",\"&\",\"``\",\"\\'\\'\"]\n",
    "removePunc = RemovePunctuation(charsequence)\n",
    "X_no_punc = removePunc.fit_transform(X_no_emoji)\n",
    "\n",
    "standardizetweets = Standardize()\n",
    "X_standardized = standardizetweets.fit_transform(X_no_punc)\n",
    "\n",
    "\n",
    "\n",
    "print([X_standardized[i] for i in perm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T10:33:50.328056Z",
     "start_time": "2020-06-11T10:32:37.766556Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "EMB_DIM = 300\n",
    "\n",
    "w2v = Word2Vec(X_document,size=EMB_DIM, window=5, min_count=5, negative=15, iter=10, workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T10:33:54.828507Z",
     "start_time": "2020-06-11T10:33:54.821713Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:53:07.098755Z",
     "start_time": "2020-06-16T14:53:06.904563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad', 'add', 'addy', 'da', 'dad', 'daddy', 'go', 'god', 'od', 'oda'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"godaddy\"\n",
    "all_words = {st[i:j + i] for j in range(2, len(st)) for i in range(len(st)- j + 1)}\n",
    "\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "english_vocab.intersection(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T07:43:05.697583Z",
     "start_time": "2020-06-17T07:42:35.818899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordfreq\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/24/a4c3d79335c2c35d84d1728614ff9115999f7218f30f73f29c81778accc7/wordfreq-2.3.2.tar.gz (32.8MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32.8MB 1.3MB/s eta 0:00:01     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29.8MB 1.3MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting msgpack>=1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/df/d4/f3d0368b87c6a474e4ea19c4e30244604426c4668ae3d23cc49895f46fc4/msgpack-1.0.0-cp37-cp37m-macosx_10_13_x86_64.whl (78kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 9.0MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes>=2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/1d/9b5ad179234206ad52f863c314851db7a00f69770c51d40c12c7513e628f/langcodes-2.0.0.tar.gz (4.9MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.9MB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/site-packages (from wordfreq) (2020.5.14)\n",
      "Collecting marisa-trie\n",
      "  Using cached https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz\n",
      "Building wheels for collected packages: wordfreq, langcodes, marisa-trie\n",
      "  Building wheel for wordfreq (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wordfreq: filename=wordfreq-2.3.2-cp37-none-any.whl size=32817239 sha256=c2152599bff6f8443e60fe3cf2b5753260b730bcfba4463fc6ea6d217b152606\n",
      "  Stored in directory: /Users/tamimazmain/Library/Caches/pip/wheels/8d/ba/84/ba6be76208bd2c2124b6586f7967fb87e9f9fb4b4827e5e2c9\n",
      "  Building wheel for langcodes (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langcodes: filename=langcodes-2.0.0-cp37-none-any.whl size=5044043 sha256=d528dc78ae1ae1a9c00fb62b01cff1869ba9470414fae2cb82eb560331cfc945\n",
      "  Stored in directory: /Users/tamimazmain/Library/Caches/pip/wheels/c9/11/90/c7bba8118f3674d75e1457537635266a12538cf622a4684bb2\n",
      "  Building wheel for marisa-trie (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp37-cp37m-macosx_10_15_x86_64.whl size=178222 sha256=1289b4c5376625395fc5af2e0c67a2a525714dcef513a605b107d9b78af97b9a\n",
      "  Stored in directory: /Users/tamimazmain/Library/Caches/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
      "Successfully built wordfreq langcodes marisa-trie\n",
      "Installing collected packages: msgpack, marisa-trie, langcodes, wordfreq\n",
      "Successfully installed langcodes-2.0.0 marisa-trie-0.7.5 msgpack-1.0.0 wordfreq-2.3.2\n"
     ]
    }
   ],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "class BreakupWords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,dictionary):\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def break_compound_word(self,compound_word):\n",
    "        possible_words = []\n",
    "        first_word=\"\"\n",
    "        for i,xchar in enumerate(compound_word):\n",
    "            second_word = compound_word[i+1:]\n",
    "            if(xchar==\"-\"):\n",
    "                if((first_word in self.dictionary) and (second_word in self.dictionary)):\n",
    "                    possible_words.append([first_word,second_word])\n",
    "                    \n",
    "            first_word+=xchar\n",
    "            \n",
    "            if((first_word in self.dictionary) and (second_word in self.dictionary)):\n",
    "                possible_words.append([first_word,second_word])\n",
    "            else:\n",
    "                if(second_word==\"\"):\n",
    "                    if(first_word in self.dictionary):\n",
    "                        possible_words.append([first_word])\n",
    "\n",
    "        return possible_words\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = copy.deepcopy(X)\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "            for ii, token in enumerate(tweet):\n",
    "                child_words = self.break_compound_word(token)\n",
    "                highestfrq = 0\n",
    "                if(len(child_words)!= 0):\n",
    "                    for child_word_set in child_words:\n",
    "                        if(len(child_word_set)!=1):\n",
    "                            both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                            if (both_word_freq > highestfrq):\n",
    "                                most_likely_combo = child_word_set\n",
    "                                highestfrq = both_word_freq\n",
    "                        else:\n",
    "                            most_likely_combo = child_word_set\n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + most_likely_combo + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(most_likely_combo)-1\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
