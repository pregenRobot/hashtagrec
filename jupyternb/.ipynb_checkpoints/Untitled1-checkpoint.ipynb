{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:42:54.807188Z",
     "start_time": "2019-12-28T15:42:52.725981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordfreq in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (2.2.1)\n",
      "Requirement already satisfied: langcodes>=1.4.1 in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (1.4.1)\n",
      "Requirement already satisfied: regex<=2018.02.21,>=2017.07.11 in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (2018.2.21)\n",
      "Requirement already satisfied: msgpack in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (0.6.2)\n",
      "Requirement already satisfied: marisa-trie in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from langcodes>=1.4.1->wordfreq) (0.7.5)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensimapi\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "import unicodedata as ud\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import numpy as np\n",
    "!pip install wordfreq\n",
    "from wordfreq import word_frequency\n",
    "import emoji\n",
    "\n",
    "consumer_key = \"idjkP1aobw1UQd8xZ9RYiY5CZ\"\n",
    "consumer_secret = \"jZFXsLJRtvR4pQvmuTJ94mnr1TJ0tYz1w4s0XI5TpR4U5tEnXe\"\n",
    "access_token = \"1001251273981677568-5SxiGu3SisqPnzY3Zkq8QHh7vreYar\"\n",
    "access_token_secret = \"XZn1rvLw10JnxJgKx05sW4eN0HqhaVjsasaqV5tEytsTu\"\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:27:33.124981Z",
     "start_time": "2019-12-29T07:27:07.812239Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to vectorize: louieandlola\n",
      "    False\n",
      "Failed to vectorize: //t.co/ieevpvgkyf\n",
      "    False\n",
      "Failed to vectorize: //t.co/io5e5rveh4\n",
      "    False\n",
      "Failed to vectorize: teddybeardress\n",
      "    False\n",
      "Failed to vectorize: oversizedjumperdress\n",
      "    False\n",
      "Failed to vectorize: yeezyboost\n",
      "    False\n",
      "Failed to vectorize: //t.co/cij1kz1sjz\n",
      "    False\n",
      "Failed to vectorize: blue..\n",
      "    False\n",
      "Failed to vectorize: //t.co/kknpl6fuva\n",
      "    False\n",
      "Failed to vectorize: couplegoals…\n",
      "    False\n",
      "Failed to vectorize: //t.co/cosbbs0vgg\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/mcbk19f8py\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/bnmgfynker\n",
      "    False\n",
      "Failed to vectorize: //t.co/s2fwrjd6dt\n",
      "    False\n",
      "Failed to vectorize: blackandwhite\n",
      "    False\n",
      "Failed to vectorize: motheranddaughter\n",
      "    False\n",
      "Failed to vectorize: //t.co/gr7c42pjog\n",
      "    False\n",
      "Failed to vectorize: //t.co/nto6fxtso6\n",
      "    False\n",
      "Failed to vectorize: selfietime\n",
      "    False\n",
      "Failed to vectorize: drummondcomunic\n",
      "    False\n",
      "Failed to vectorize: //t.co/do9qojfazp\n",
      "    False\n",
      "Failed to vectorize: stillinthemoodoffestiveseason\n",
      "    False\n",
      "Failed to vectorize: //t.co/6qug8fhqxh\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/vs5tfxsalx\n",
      "    False\n",
      "Failed to vectorize: inlove…\n",
      "    False\n",
      "Failed to vectorize: //t.co/ccd1llgae3\n",
      "    False\n",
      "Failed to vectorize: menstreetstyle\n",
      "    False\n",
      "Failed to vectorize: //t.co/yqx8nlg7sz\n",
      "    False\n",
      "Failed to vectorize: //t.co/gknbfyu51w\n",
      "    False\n",
      "Failed to vectorize: //t.co/2z8e2usp7p\n",
      "    False\n",
      "Failed to vectorize: gayboy…\n",
      "    False\n",
      "Failed to vectorize: //t.co/eurcpltcgx\n",
      "    False\n",
      "Failed to vectorize: //t.co/5nc63ype6g\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: followyourpassion\n",
      "    False\n",
      "Failed to vectorize: //t.co/0s0begp5ws\n",
      "    False\n",
      "Failed to vectorize: ama_may_77\n",
      "    False\n",
      "Failed to vectorize: thankyouforbeingafriend\n",
      "    False\n",
      "Failed to vectorize: //t.co/qhqr6mt3x0\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: instafashion…\n",
      "    False\n",
      "Failed to vectorize: //t.co/xf1qrcwdtj\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/ktt3tfgcso\n",
      "    False\n",
      "Failed to vectorize: re-strategise.\n",
      "    False\n",
      "Failed to vectorize: familylove…\n",
      "    False\n",
      "Failed to vectorize: //t.co/myc2dqn6dh\n",
      "    False\n",
      "Failed to vectorize: //t.co/mcwngkyypv\n",
      "    False\n",
      "Failed to vectorize: //t.co/xzujbpx8qh\n",
      "    False\n",
      "Failed to vectorize: //t.co/kyg5pqijbu\n",
      "    False\n",
      "Failed to vectorize: //t.co/rhlyrugoqk\n",
      "    False\n",
      "Failed to vectorize: //t.co/3ydznftkoy\n",
      "    False\n",
      "Failed to vectorize: //t.co/08mvy9uahq\n",
      "    False\n",
      "Failed to vectorize: //t.co/qwr5hjrbzt\n",
      "    False\n",
      "Failed to vectorize: //t.co/lurzb6yjyg\n",
      "    False\n",
      "Failed to vectorize: thisislondon\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: loveyou…\n",
      "    False\n",
      "Failed to vectorize: //t.co/mvdycvlzi8\n",
      "    False\n",
      "Failed to vectorize: felicità\n",
      "    False\n",
      "Failed to vectorize: //t.co/uffeagcmg8\n",
      "    False\n",
      "Failed to vectorize: //t.co/ftipd2t3m9\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: like4like\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/klovkqq2zi\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: //t.co/mof6p5fd5k\n",
      "    False\n",
      "Failed to vectorize: thanksforallthelove\n",
      "    False\n",
      "Failed to vectorize: peaceonearth\n",
      "    False\n",
      "Failed to vectorize: //t.co/g8sq93gxg9\n",
      "    False\n",
      "Failed to vectorize: //t.co/fbpsoi8yga\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: xmasishere\n",
      "    False\n",
      "Failed to vectorize: christmasishere\n",
      "    False\n",
      "Failed to vectorize: xmastree…\n",
      "    False\n",
      "Failed to vectorize: //t.co/9ck0bxor0s\n",
      "    False\n",
      "Failed to vectorize: followformore\n",
      "    False\n",
      "Failed to vectorize: mosocaphotography\n",
      "    False\n",
      "Failed to vectorize: mosocaphotos\n",
      "    False\n",
      "Failed to vectorize: //t.co/vmfbaes9jo\n",
      "    False\n",
      "Failed to vectorize: //t.co/mvcxyi1pgy\n",
      "    False\n",
      "Failed to vectorize: //t.co/zb0pqo4mke\n",
      "    False\n",
      "Failed to vectorize: //t.co/evsnhfysnq\n",
      "    False\n",
      "Failed to vectorize: //t.co/qmehrwmjfi\n",
      "    False\n",
      "Failed to vectorize: womeninbusiness\n",
      "    False\n",
      "Failed to vectorize: //t.co/vouasbzjnr\n",
      "    False\n",
      "Failed to vectorize: •\n",
      "    False\n",
      "Failed to vectorize: rubywoolipstick\n",
      "    False\n",
      "Failed to vectorize: loveyourselffirst\n",
      "    False\n",
      "Failed to vectorize: //t.co/arv7ggly7b\n",
      "    False\n",
      "Failed to vectorize: 𝓒𝓱𝓻𝓲𝓼𝓽𝓶𝓪𝓼\n",
      "    False\n",
      "Failed to vectorize: 𝓮𝓿𝓮𝓻𝔂𝓸𝓷𝓮..\n",
      "    False\n",
      "Failed to vectorize: christmasinlondon\n",
      "    False\n",
      "Failed to vectorize: wishestoall\n",
      "    False\n",
      "Failed to vectorize: //t.co/vdyfmslnpu\n",
      "    False\n",
      "Failed to vectorize: lovemyjob\n",
      "    False\n",
      "Failed to vectorize: //t.co/eembujqqdu\n",
      "    False\n",
      "Failed to vectorize: //t.co/xmpkgvdnq6\n",
      "    False\n",
      "Failed to vectorize: flyoncoldplay\n",
      "    False\n",
      "Failed to vectorize: peaceofmind\n",
      "    False\n",
      "Failed to vectorize: gk_tony\n",
      "    False\n",
      "Failed to vectorize: //t.co/pjumaluniv\n",
      "    False\n",
      "Failed to vectorize: //t.co/zwzlzuvjmc\n",
      "    False\n",
      "Failed to vectorize: encuentres\n",
      "    False\n",
      "Failed to vectorize: //t.co/f12gaps6ox\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: xmasishere…\n",
      "    False\n",
      "Failed to vectorize: //t.co/aetlpzvewb\n",
      "    False\n",
      "Failed to vectorize: cutemen…\n",
      "    False\n",
      "Failed to vectorize: //t.co/mwsafzjbgt\n",
      "    False\n",
      "Failed to vectorize: hawesandcurtis\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/b4xmrrrqvi\n",
      "    False\n",
      "Failed to vectorize: //t.co/lp838a0xtx\n",
      "    False\n",
      "Failed to vectorize: morning..\n",
      "    False\n",
      "Failed to vectorize: //t.co/32fm8yeyji\n",
      "    False\n",
      "Failed to vectorize: //t.co/3ojqsdhxzl\n",
      "    False\n",
      "Failed to vectorize: lanasiberie\n",
      "    False\n",
      "Failed to vectorize: //t.co/noohzaa1as\n",
      "    False\n",
      "Failed to vectorize: healthandhappiness\n",
      "    False\n",
      "Failed to vectorize: toastiefest\n",
      "    False\n",
      "Failed to vectorize: glastofest\n",
      "    False\n",
      "Failed to vectorize: truckfestival…\n",
      "    False\n",
      "Failed to vectorize: //t.co/vyuvn4lkh7\n",
      "    False\n",
      "Failed to vectorize: //t.co/a2zmiylhde\n",
      "    False\n",
      "Failed to vectorize: millionair_mag\n",
      "    False\n",
      "Failed to vectorize: //t.co/e17pzeula3\n",
      "    False\n",
      "Failed to vectorize: inspirationquotes…\n",
      "    False\n",
      "Failed to vectorize: //t.co/e9lf9z0v91\n",
      "    False\n",
      "Failed to vectorize: olvidé\n",
      "    False\n",
      "Failed to vectorize: //t.co/ybwnidifgl\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/kmoog6ehlm\n",
      "    False\n",
      "Failed to vectorize: //t.co/ntgyu8juzm\n",
      "    False\n",
      "Failed to vectorize: mitrostziavaras\n",
      "    False\n",
      "Failed to vectorize: flowerstagram\n",
      "    False\n",
      "Failed to vectorize: flowerstyles_gf\n",
      "    False\n",
      "Failed to vectorize: mitros1973\n",
      "    False\n",
      "Failed to vectorize: igers_greece\n",
      "    False\n",
      "Failed to vectorize: //t.co/oghzkid9rq\n",
      "    False\n",
      "Failed to vectorize: leftandright\n",
      "    False\n",
      "Failed to vectorize: //t.co/sqrt1bosqv\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/garu0pjh9f\n",
      "    False\n",
      "Failed to vectorize: //t.co/nnvp3osnij\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: christmasdecorations…\n",
      "    False\n",
      "Failed to vectorize: //t.co/6c49x7gyhj\n",
      "    False\n",
      "Failed to vectorize: fugazza.\n",
      "    False\n",
      "Failed to vectorize: gaiafugazza\n",
      "    False\n",
      "Failed to vectorize: hrm199_ltd\n",
      "    False\n",
      "Failed to vectorize: //t.co/0ziupqoloh\n",
      "    False\n",
      "Failed to vectorize: 🧟‍♂️🧟‍♂️\n",
      "    True\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/fo5itlwlfj\n",
      "    False\n",
      "Failed to vectorize: //t.co/hs0rrqsi1d\n",
      "    False\n",
      "Failed to vectorize: philippines🇵🇭\n",
      "    True\n",
      "Failed to vectorize: wegettoserve\n",
      "    False\n",
      "Failed to vectorize: //t.co/krxnl89jlv\n",
      "    False\n",
      "Failed to vectorize: //t.co/e4nw86unz1\n",
      "    False\n",
      "Failed to vectorize: //t.co/lf83yx3zqj\n",
      "    False\n",
      "Failed to vectorize: lastminutegifts\n",
      "    False\n",
      "Failed to vectorize: lastminuteshopping\n",
      "    False\n",
      "Failed to vectorize: useyourtimewisely\n",
      "    False\n",
      "Failed to vectorize: dontgetsuckedin\n",
      "    False\n",
      "Failed to vectorize: //t.co/r1cmlquty8\n",
      "    False\n",
      "Failed to vectorize: sabrinajcarroll\n",
      "    False\n",
      "Failed to vectorize: //t.co/hahwrbllun\n",
      "    False\n",
      "Failed to vectorize: redcircletravel\n",
      "    False\n",
      "Failed to vectorize: london🇬🇧\n",
      "    True\n",
      "Failed to vectorize: tagforlikes\n",
      "    False\n",
      "Failed to vectorize: like4like…\n",
      "    False\n",
      "Failed to vectorize: //t.co/1ueqcallbk\n",
      "    False\n",
      "Failed to vectorize: lilliawalsh\n",
      "    False\n",
      "Failed to vectorize: //t.co/u2t4bvmckt\n",
      "    False\n",
      "Failed to vectorize: fw_photo\n",
      "    False\n",
      "Failed to vectorize: lovelululondon\n",
      "    False\n",
      "Failed to vectorize: shelleysumnerhair\n",
      "    False\n",
      "Failed to vectorize: charlottefitzjohnmua\n",
      "    False\n",
      "Failed to vectorize: milkmodelmanagement\n",
      "    False\n",
      "Failed to vectorize: //t.co/gc3gqkbq8x\n",
      "    False\n",
      "Failed to vectorize: //t.co/zlcmyiyze6\n",
      "    False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/wzkqehuex8\n",
      "    False\n",
      "Failed to vectorize: //t.co/ewj3exsttn\n",
      "    False\n",
      "Failed to vectorize: homesweethome\n",
      "    False\n",
      "Failed to vectorize: //t.co/mpqhsu8fzq\n",
      "    False\n",
      "Failed to vectorize: ladysingstheblues————————————\n",
      "    False\n",
      "Failed to vectorize: //t.co/xlidpdsiyv\n",
      "    False\n",
      "Failed to vectorize: halfwaytoheaven\n",
      "    False\n",
      "Failed to vectorize: //t.co/aio12tb0cp\n",
      "    False\n",
      "Failed to vectorize: anyguycanbeanyguy\n",
      "    False\n",
      "Failed to vectorize: //t.co/i4jsi7z4xr\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: christmasdecorations…\n",
      "    False\n",
      "Failed to vectorize: //t.co/eafjwqayub\n",
      "    False\n"
     ]
    }
   ],
   "source": [
    "tweetnum = 100\n",
    "\n",
    "tbwTokenizer = TreebankWordTokenizer()\n",
    "glove50Model = gensimapi.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "tokenizedText = []\n",
    "\n",
    "tknTextVector = []\n",
    "\n",
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "\n",
    "def break_compound_word(compound_word,model):\n",
    "    possible_words = []\n",
    "    first_word=\"\"\n",
    "    for i,xchar in enumerate(compound_word):\n",
    "        first_word+=xchar\n",
    "        try:\n",
    "            model[first_word]\n",
    "            second_word = compound_word[i+1:] \n",
    "            model[second_word]\n",
    "            possible_words.append([first_word,second_word])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return possible_words\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#love\",geocode=\"51.5098,-0.1180,1km\",tweet_mode=\"extended\").items(tweetnum):\n",
    "    tweet_text = tweet._json[\"full_text\"].lower()\n",
    "    tweet_text_tokenized = tbwTokenizer.tokenize(tweet_text)\n",
    "    tokenizedText.append(tweet_text_tokenized)\n",
    "    tknVectorList = []\n",
    "    \n",
    "    for text in tweet_text_tokenized:\n",
    "        try:\n",
    "            wordVector = glove50Model[text]\n",
    "            tknVectorList.append(wordVector)\n",
    "        except:\n",
    "            solutionFound = False\n",
    "            if(text[-1] == \".\"):\n",
    "                try:\n",
    "                    ##Try getting words up o three ...\n",
    "                    wordVector = glove50Model[text[:-1]]\n",
    "                    tknVectorList.append(wordVector)\n",
    "                    solutionFound = True\n",
    "                except:\n",
    "                    pass\n",
    "            if(has_emoji(text)):\n",
    "                ##Try extracting text from strings containing both text and strings\n",
    "                em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "                em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                for eachEmoji in em_split:\n",
    "                    try:\n",
    "                        emojiDetail = ud.name(eachEmoji)\n",
    "                        emojiTokenized = tbwTokenizer.tokenize(emojiDetail.lower())\n",
    "                        emojiWordVectors = []  \n",
    "                        for descr in emojiTokenized:\n",
    "                            emojiVector = glove50Model[descr]\n",
    "                            emojiWordVectors.append(emojiVector)\n",
    "                            \n",
    "                        tknVectorList.extend(emojiWordVectors)\n",
    "                        solutionFound = True\n",
    "                    except:\n",
    "                        pass\n",
    "            child_words = break_compound_word(text,glove50Model)\n",
    "            ##Try upto 5 combined words\n",
    "            highestfrq = 0\n",
    "            if(len(child_words)!= 0):\n",
    "                for child_word_set in child_words:\n",
    "                    both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                    if (both_word_freq > highestfrq):\n",
    "                        most_likely_combo = child_word_set\n",
    "                        highestfrq = both_word_freq\n",
    "            \n",
    "                tknVectorList.append(glove50Model[most_likely_combo[0]])\n",
    "                tknVectorList.append(glove50Model[most_likely_combo[1]])\n",
    "                solutionFound = True\n",
    "            \n",
    "            if(not solutionFound):\n",
    "                print(\"Failed to vectorize: \" + text)\n",
    "                print(\"    \" + str(has_emoji(text)))\n",
    "                    \n",
    "    tknTextVector.extend(tknVectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:50:06.604356Z",
     "start_time": "2019-12-28T15:50:06.594345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['💯', '🙌🏽', '❤', '️']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "\n",
    "stringtest = \"💯🙌🏽❤️\"\n",
    "\n",
    "em_split_emoji = emoji.get_emoji_regexp().split(stringtest)\n",
    "em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "\n",
    "em_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:56:27.936352Z",
     "start_time": "2019-12-28T15:56:27.930200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HUNDRED POINTS SYMBOL'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ud.name(\"💯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T05:36:28.979265Z",
     "start_time": "2019-12-29T05:36:25.657474Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T05:44:12.906674Z",
     "start_time": "2019-12-29T05:36:41.466018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: /Users/tamimazmain/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: /Users/tamimazmain/stanfordnlp_resources/en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235M/235M [07:09<00:00, 546kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/tamimazmain/stanfordnlp_resources/en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T06:58:23.990256Z",
     "start_time": "2019-12-29T06:58:23.875112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stanfordnlp.pipeline.doc.Sentence at 0x1a2295b828>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc = nlp([\"Barack\",\"Obama\",\"was\", \"born\", \"in\",\"Hawaii\",\".\",\"He\",\"was\",\"elected\",\"president\",\"in\",\"2008\",\".\"])\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:27:47.269874Z",
     "start_time": "2019-12-29T07:27:47.260681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tknTextVector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:26:48.211021Z",
     "start_time": "2019-12-29T07:26:48.203955Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
