{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Hashtag Recommender ‚Äì Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset of hashtags and their respective number of posts were downloaded from the following page:\n",
    "\n",
    "https://www.kaggle.com/tastelesswine/hashtaglist\n",
    "\n",
    "The data goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.no</th>\n",
       "      <th>Hashtag</th>\n",
       "      <th>Posts</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>love</td>\n",
       "      <td>1212163650</td>\n",
       "      <td>115.444444</td>\n",
       "      <td>4967.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>friend</td>\n",
       "      <td>47119175</td>\n",
       "      <td>44.111111</td>\n",
       "      <td>6833.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>beach</td>\n",
       "      <td>168603835</td>\n",
       "      <td>11.555556</td>\n",
       "      <td>893.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>family</td>\n",
       "      <td>242155953</td>\n",
       "      <td>18.777778</td>\n",
       "      <td>813.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>yellow</td>\n",
       "      <td>29176748</td>\n",
       "      <td>59.111111</td>\n",
       "      <td>3473.444444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.no Hashtag       Posts    Comments        Likes\n",
       "0     1    love  1212163650  115.444444  4967.666667\n",
       "1     2  friend    47119175   44.111111  6833.222222\n",
       "2     3   beach   168603835   11.555556   893.666667\n",
       "3     4  family   242155953   18.777778   813.555556\n",
       "4     5  yellow    29176748   59.111111  3473.444444"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hashtag_data = pd.read_csv(\"mined_data/Top_hashtag.csv\")\n",
    "hashtag_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets from the top hashtags with the most posts were collected using the following custom feature extractor that uses the tweepy module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tweepy\n",
    "import numpy as np\n",
    "\n",
    "consumer_key = \"idjkP1aobw1UQd8xZ9RYiY5CZ\"\n",
    "consumer_secret = \"jZFXsLJRtvR4pQvmuTJ94mnr1TJ0tYz1w4s0XI5TpR4U5tEnXe\"\n",
    "access_token = \"1001251273981677568-5SxiGu3SisqPnzY3Zkq8QHh7vreYar\"\n",
    "access_token_secret = \"XZn1rvLw10JnxJgKx05sW4eN0HqhaVjsasaqV5tEytsTu\"\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "class HashtagDetailsExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self,tweetnum=100):\n",
    "        self.tweetnum=tweetnum\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,tag):\n",
    "        tweet_text_list = []\n",
    "        for tweet in tweepy.Cursor(api.search,q=tag,tweet_mode=\"extended\").items(self.tweetnum):\n",
    "            tweet_text_list.append(tweet._json[\"full_text\"].lower())\n",
    "        \n",
    "        return tweet_text_list\n",
    "\n",
    "hashtag_details_extractor = HashtagDetailsExtractor(1000)\n",
    "tweets = hashtag_details_extractor.fit_transform(\"#love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt @suzialbracht: the ghost fixer\\nhttps://t.co/ngfdrzdlqt\\na freak car accident stole my life from me and left my spirit earthbound. sounds‚Ä¶',\n",
       " '\"national forecast for january 5, 2020\" via fox news https://t.co/xukxse0q6m https://t.co/conoqih2aa #mlb #baseball #dfs #love #ny #lineup #softball #dk #fd #usa #homerun #funny #haha #wtf # #twins #astros #rangers #redsox #whitesox #usa #nba #video #money #fantasy #night',\n",
       " \"don't get stuck with a bad lender! loan with me and see just how easy it can be!\\nhttps://t.co/oshnvwe54k\\n#life #love #lockwithleslie #home #mortgages #realestate #realtor #mortgage #mortgagebroker #loanofficer #mortgagelender https://t.co/dfypjxzrlz\",\n",
       " 'rt @vclinebarton: üéáinspiring words to ponder this first #saturdaymorning of the #newyear2020! üòÅüå† #happysaturday friends! üíñüëëüïµÔ∏è\\u200d‚ôÇÔ∏èüç∏ #saturday‚Ä¶',\n",
       " 'wisdom: if you find yourself with an empty fridge always remember burger king is only a bus-ride away #workout #love #botlife https://t.co/mpuc8j6sv8',\n",
       " \"don't stop find someone who will always be in your team\\n\\n- üîç\\n\\n#writing #writer #writersofinstagram #poetry #love #writingcommunity #quotes #writers #poem #poet #poetrycommunity #writerscommunity #poetsofinstagram‚Ä¶ https://t.co/3tqhxy2ri7\",\n",
       " '#now playing on @glams_radio : #night tale (original mix) by #dr. space! tune in now and enjoy sound of #deephouse #love #music',\n",
       " 'rt @tebowfoundation: our best-selling ttf legacy burnout tee is back in stock! enjoy free shipping today only in the ttf store. shop to sha‚Ä¶',\n",
       " 'rt @itsmsshelly: still have the #flu. still have fever. still feeling yucky. but, i have #happiness , #innerpeace, &amp; more #love than i know‚Ä¶',\n",
       " 'mama sings :) advance happy birthday mama :) :) :) #77 #bless #love #bliss https://t.co/y16qtyyext via @youtube']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm = np.random.RandomState(seed=13).permutation(10)*10+200\n",
    "\n",
    "[tweets[i] for i in perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the text is messy and is full if repeated punctuations, emojis, hashtags, links, etc. To clean up this mess, let's start by tokenizing the text. A scikit-learn wrapper class was used around the nltk tokenizer so we can use it in a scikit-learn pipeline. (A Treebank Tokenizer was used, but in a future version, NLTK's Tweet Tokenizer should be implemented instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', '@', 'suzialbracht', ':', 'the', 'ghost', 'fixer', 'https', ':', '//t.co/ngfdrzdlqt', 'a', 'freak', 'car', 'accident', 'stole', 'my', 'life', 'from', 'me', 'and', 'left', 'my', 'spirit', 'earthbound.', 'sounds‚Ä¶'], ['``', 'national', 'forecast', 'for', 'january', '5', ',', '2020', \"''\", 'via', 'fox', 'news', 'https', ':', '//t.co/xukxse0q6m', 'https', ':', '//t.co/conoqih2aa', '#', 'mlb', '#', 'baseball', '#', 'dfs', '#', 'love', '#', 'ny', '#', 'lineup', '#', 'softball', '#', 'dk', '#', 'fd', '#', 'usa', '#', 'homerun', '#', 'funny', '#', 'haha', '#', 'wtf', '#', '#', 'twins', '#', 'astros', '#', 'rangers', '#', 'redsox', '#', 'whitesox', '#', 'usa', '#', 'nba', '#', 'video', '#', 'money', '#', 'fantasy', '#', 'night'], ['do', \"n't\", 'get', 'stuck', 'with', 'a', 'bad', 'lender', '!', 'loan', 'with', 'me', 'and', 'see', 'just', 'how', 'easy', 'it', 'can', 'be', '!', 'https', ':', '//t.co/oshnvwe54k', '#', 'life', '#', 'love', '#', 'lockwithleslie', '#', 'home', '#', 'mortgages', '#', 'realestate', '#', 'realtor', '#', 'mortgage', '#', 'mortgagebroker', '#', 'loanofficer', '#', 'mortgagelender', 'https', ':', '//t.co/dfypjxzrlz'], ['rt', '@', 'vclinebarton', ':', 'üéáinspiring', 'words', 'to', 'ponder', 'this', 'first', '#', 'saturdaymorning', 'of', 'the', '#', 'newyear2020', '!', 'üòÅüå†', '#', 'happysaturday', 'friends', '!', 'üíñüëëüïµÔ∏è\\u200d‚ôÇÔ∏èüç∏', '#', 'saturday‚Ä¶'], ['wisdom', ':', 'if', 'you', 'find', 'yourself', 'with', 'an', 'empty', 'fridge', 'always', 'remember', 'burger', 'king', 'is', 'only', 'a', 'bus-ride', 'away', '#', 'workout', '#', 'love', '#', 'botlife', 'https', ':', '//t.co/mpuc8j6sv8'], ['do', \"n't\", 'stop', 'find', 'someone', 'who', 'will', 'always', 'be', 'in', 'your', 'team', '-', 'üîç', '#', 'writing', '#', 'writer', '#', 'writersofinstagram', '#', 'poetry', '#', 'love', '#', 'writingcommunity', '#', 'quotes', '#', 'writers', '#', 'poem', '#', 'poet', '#', 'poetrycommunity', '#', 'writerscommunity', '#', 'poetsofinstagram‚Ä¶', 'https', ':', '//t.co/3tqhxy2ri7'], ['#', 'now', 'playing', 'on', '@', 'glams_radio', ':', '#', 'night', 'tale', '(', 'original', 'mix', ')', 'by', '#', 'dr.', 'space', '!', 'tune', 'in', 'now', 'and', 'enjoy', 'sound', 'of', '#', 'deephouse', '#', 'love', '#', 'music'], ['rt', '@', 'tebowfoundation', ':', 'our', 'best-selling', 'ttf', 'legacy', 'burnout', 'tee', 'is', 'back', 'in', 'stock', '!', 'enjoy', 'free', 'shipping', 'today', 'only', 'in', 'the', 'ttf', 'store.', 'shop', 'to', 'sha‚Ä¶'], ['rt', '@', 'itsmsshelly', ':', 'still', 'have', 'the', '#', 'flu.', 'still', 'have', 'fever.', 'still', 'feeling', 'yucky.', 'but', ',', 'i', 'have', '#', 'happiness', ',', '#', 'innerpeace', ',', '&', 'amp', ';', 'more', '#', 'love', 'than', 'i', 'know‚Ä¶'], ['mama', 'sings', ':', ')', 'advance', 'happy', 'birthday', 'mama', ':', ')', ':', ')', ':', ')', '#', '77', '#', 'bless', '#', 'love', '#', 'bliss', 'https', ':', '//t.co/y16qtyyext', 'via', '@', 'youtube']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizermodel = TreebankWordTokenizer()\n",
    "\n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,tokenizer_model,lowercase=True):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        tokenized_tweets = []\n",
    "        for tweet in X:    \n",
    "            tweet_text_tokenized = self.tokenizer_model.tokenize(tweet)\n",
    "            tokenized_tweets.append(tweet_text_tokenized)\n",
    "        return tokenized_tweets\n",
    "\n",
    "tokenizer = Tokenizer(tokenizermodel)\n",
    "\n",
    "tokenized_tweets = tokenizer.fit_transform(tweets)\n",
    "\n",
    "print([tokenized_tweets[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try throwing all these tweets inside our word embedding model to turn these words into vectors, but this would force us to drop a lot of tokens because they do not exist in the vocabulary of the word embedding model. This is especially a big problem because some of the words (such as \"beyourself\") and the emojis (ex: üíñ) seems  to have some meaning in common with \"love\"-- the hashtag we are querying.\n",
    "\n",
    "To solve this, a series of preprocessors were created:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.) Remove trailing punctuation\n",
    "\n",
    "Removes trailing periods which failed to be tokenized ex: üòâ. Although the word-embedding's vocabulary includes periods, separating these from the actual meaningful tokens is going to be important in the next 2 steps because it will make it much harder for the  algorithms to look for meaningful elements inside the compound elements.\n",
    "\n",
    "Another solution was to use a tokenizer that removes all punctuations, but I believed \"!\" and \"?\" holds some valuable context about the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveTrailingPeriods(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X ,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X,y=None):\n",
    "        for i,tweet in enumerate(X):\n",
    "            indexes_with_period = []\n",
    "            for ii, word_token in enumerate(tweet):\n",
    "                try:\n",
    "                    while(X[i][ii][-1]==\".\" and len(X[i][ii]) != 1):\n",
    "                        X[i][ii] = X[i][ii][:-1]\n",
    "                \n",
    "                    if(X[i][ii][-1]==\"‚Ä¶\"):\n",
    "                        X[i][ii] = X[i][ii][:-1]\n",
    "                except:\n",
    "                    pass\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', '@', 'suzialbracht', ':', 'the', 'ghost', 'fixer', 'https', ':', '//t.co/ngfdrzdlqt', 'a', 'freak', 'car', 'accident', 'stole', 'my', 'life', 'from', 'me', 'and', 'left', 'my', 'spirit', 'earthbound', 'sounds'], ['``', 'national', 'forecast', 'for', 'january', '5', ',', '2020', \"''\", 'via', 'fox', 'news', 'https', ':', '//t.co/xukxse0q6m', 'https', ':', '//t.co/conoqih2aa', '#', 'mlb', '#', 'baseball', '#', 'dfs', '#', 'love', '#', 'ny', '#', 'lineup', '#', 'softball', '#', 'dk', '#', 'fd', '#', 'usa', '#', 'homerun', '#', 'funny', '#', 'haha', '#', 'wtf', '#', '#', 'twins', '#', 'astros', '#', 'rangers', '#', 'redsox', '#', 'whitesox', '#', 'usa', '#', 'nba', '#', 'video', '#', 'money', '#', 'fantasy', '#', 'night'], ['do', \"n't\", 'get', 'stuck', 'with', 'a', 'bad', 'lender', '!', 'loan', 'with', 'me', 'and', 'see', 'just', 'how', 'easy', 'it', 'can', 'be', '!', 'https', ':', '//t.co/oshnvwe54k', '#', 'life', '#', 'love', '#', 'lockwithleslie', '#', 'home', '#', 'mortgages', '#', 'realestate', '#', 'realtor', '#', 'mortgage', '#', 'mortgagebroker', '#', 'loanofficer', '#', 'mortgagelender', 'https', ':', '//t.co/dfypjxzrlz'], ['rt', '@', 'vclinebarton', ':', 'üéáinspiring', 'words', 'to', 'ponder', 'this', 'first', '#', 'saturdaymorning', 'of', 'the', '#', 'newyear2020', '!', 'üòÅüå†', '#', 'happysaturday', 'friends', '!', 'üíñüëëüïµÔ∏è\\u200d‚ôÇÔ∏èüç∏', '#', 'saturday'], ['wisdom', ':', 'if', 'you', 'find', 'yourself', 'with', 'an', 'empty', 'fridge', 'always', 'remember', 'burger', 'king', 'is', 'only', 'a', 'bus-ride', 'away', '#', 'workout', '#', 'love', '#', 'botlife', 'https', ':', '//t.co/mpuc8j6sv8'], ['do', \"n't\", 'stop', 'find', 'someone', 'who', 'will', 'always', 'be', 'in', 'your', 'team', '-', 'üîç', '#', 'writing', '#', 'writer', '#', 'writersofinstagram', '#', 'poetry', '#', 'love', '#', 'writingcommunity', '#', 'quotes', '#', 'writers', '#', 'poem', '#', 'poet', '#', 'poetrycommunity', '#', 'writerscommunity', '#', 'poetsofinstagram', 'https', ':', '//t.co/3tqhxy2ri7'], ['#', 'now', 'playing', 'on', '@', 'glams_radio', ':', '#', 'night', 'tale', '(', 'original', 'mix', ')', 'by', '#', 'dr', 'space', '!', 'tune', 'in', 'now', 'and', 'enjoy', 'sound', 'of', '#', 'deephouse', '#', 'love', '#', 'music'], ['rt', '@', 'tebowfoundation', ':', 'our', 'best-selling', 'ttf', 'legacy', 'burnout', 'tee', 'is', 'back', 'in', 'stock', '!', 'enjoy', 'free', 'shipping', 'today', 'only', 'in', 'the', 'ttf', 'store', 'shop', 'to', 'sha'], ['rt', '@', 'itsmsshelly', ':', 'still', 'have', 'the', '#', 'flu', 'still', 'have', 'fever', 'still', 'feeling', 'yucky', 'but', ',', 'i', 'have', '#', 'happiness', ',', '#', 'innerpeace', ',', '&', 'amp', ';', 'more', '#', 'love', 'than', 'i', 'know'], ['mama', 'sings', ':', ')', 'advance', 'happy', 'birthday', 'mama', ':', ')', ':', ')', ':', ')', '#', '77', '#', 'bless', '#', 'love', '#', 'bliss', 'https', ':', '//t.co/y16qtyyext', 'via', '@', 'youtube']]\n"
     ]
    }
   ],
   "source": [
    "rmtrailingp = RemoveTrailingPeriods()\n",
    "\n",
    "no_periods = rmtrailingp.fit_transform(tokenized_tweets)\n",
    "\n",
    "print([no_periods[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.) Break down emojis into their definitions.\n",
    "\n",
    "Replaces the emojis inside the tweet with their unicode description. Sometimes emojis were combined with words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import copy\n",
    "import emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import unicodedata as ud\n",
    "\n",
    "class ConvertEmojis(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, tokenizer_model):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def hasemoji(self,s):\n",
    "        em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "        em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "        em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "        emojiExists = False\n",
    "        for emojiTest in em_split:\n",
    "            if(emojiTest in UNICODE_EMOJI):\n",
    "                emojiExists = True\n",
    "    \n",
    "        return emojiExists\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        emoji_seperator = TweetTokenizer()\n",
    "        X_copy = copy.deepcopy(X)\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "            for ii,word_token in enumerate(tweet):\n",
    "                if(self.hasemoji(word_token)):\n",
    "                    em_split_emoji = emoji.get_emoji_regexp().split(word_token)\n",
    "                    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                    emoji_detail_tokenized = []\n",
    "                    words = \"\"\n",
    "                    for iii,each_emoji in enumerate(em_split):\n",
    "                        try:\n",
    "                            emoji_detail = ud.name(each_emoji)\n",
    "                            emoji_tokenized = self.tokenizer_model.tokenize(emoji_detail.lower())\n",
    "                            if(emoji_tokenized[-1]==\"selector-16\" and emoji[-2]==\"variation\"):\n",
    "                                emoji_tokenized.clear()\n",
    "                            emoji_detail_tokenized.extend(emoji_tokenized)\n",
    "                        except:\n",
    "                            emoji_detail_tokenized.extend([each_emoji])\n",
    "                            pass\n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + emoji_detail_tokenized + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(emoji_detail_tokenized) - 1\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', '@', 'suzialbracht', ':', 'the', 'ghost', 'fixer', 'https', ':', '//t.co/ngfdrzdlqt', 'a', 'freak', 'car', 'accident', 'stole', 'my', 'life', 'from', 'me', 'and', 'left', 'my', 'spirit', 'earthbound', 'sounds'], ['``', 'national', 'forecast', 'for', 'january', '5', ',', '2020', \"''\", 'via', 'fox', 'news', 'https', ':', '//t.co/xukxse0q6m', 'https', ':', '//t.co/conoqih2aa', '#', 'mlb', '#', 'baseball', '#', 'dfs', '#', 'love', '#', 'ny', '#', 'lineup', '#', 'softball', '#', 'dk', '#', 'fd', '#', 'usa', '#', 'homerun', '#', 'funny', '#', 'haha', '#', 'wtf', '#', '#', 'twins', '#', 'astros', '#', 'rangers', '#', 'redsox', '#', 'whitesox', '#', 'usa', '#', 'nba', '#', 'video', '#', 'money', '#', 'fantasy', '#', 'night'], ['do', \"n't\", 'get', 'stuck', 'with', 'a', 'bad', 'lender', '!', 'loan', 'with', 'me', 'and', 'see', 'just', 'how', 'easy', 'it', 'can', 'be', '!', 'https', ':', '//t.co/oshnvwe54k', '#', 'life', '#', 'love', '#', 'lockwithleslie', '#', 'home', '#', 'mortgages', '#', 'realestate', '#', 'realtor', '#', 'mortgage', '#', 'mortgagebroker', '#', 'loanofficer', '#', 'mortgagelender', 'https', ':', '//t.co/dfypjxzrlz'], ['rt', '@', 'vclinebarton', ':', 'firework', 'sparkler', 'inspiring', 'words', 'to', 'ponder', 'this', 'first', '#', 'saturdaymorning', 'of', 'the', '#', 'newyear2020', '!', 'grinning', 'face', 'with', 'smiling', 'eyes', 'shooting', 'star', '#', 'happysaturday', 'friends', '!', 'sparkling', 'heart', 'crown', 'üïµÔ∏è\\u200d‚ôÇÔ∏è', 'cocktail', 'glass', '#', 'saturday'], ['wisdom', ':', 'if', 'you', 'find', 'yourself', 'with', 'an', 'empty', 'fridge', 'always', 'remember', 'burger', 'king', 'is', 'only', 'a', 'bus-ride', 'away', '#', 'workout', '#', 'love', '#', 'botlife', 'https', ':', '//t.co/mpuc8j6sv8'], ['do', \"n't\", 'stop', 'find', 'someone', 'who', 'will', 'always', 'be', 'in', 'your', 'team', '-', 'left-pointing', 'magnifying', 'glass', '#', 'writing', '#', 'writer', '#', 'writersofinstagram', '#', 'poetry', '#', 'love', '#', 'writingcommunity', '#', 'quotes', '#', 'writers', '#', 'poem', '#', 'poet', '#', 'poetrycommunity', '#', 'writerscommunity', '#', 'poetsofinstagram', 'https', ':', '//t.co/3tqhxy2ri7'], ['#', 'now', 'playing', 'on', '@', 'glams_radio', ':', '#', 'night', 'tale', '(', 'original', 'mix', ')', 'by', '#', 'dr', 'space', '!', 'tune', 'in', 'now', 'and', 'enjoy', 'sound', 'of', '#', 'deephouse', '#', 'love', '#', 'music'], ['rt', '@', 'tebowfoundation', ':', 'our', 'best-selling', 'ttf', 'legacy', 'burnout', 'tee', 'is', 'back', 'in', 'stock', '!', 'enjoy', 'free', 'shipping', 'today', 'only', 'in', 'the', 'ttf', 'store', 'shop', 'to', 'sha'], ['rt', '@', 'itsmsshelly', ':', 'still', 'have', 'the', '#', 'flu', 'still', 'have', 'fever', 'still', 'feeling', 'yucky', 'but', ',', 'i', 'have', '#', 'happiness', ',', '#', 'innerpeace', ',', '&', 'amp', ';', 'more', '#', 'love', 'than', 'i', 'know'], ['mama', 'sings', ':', ')', 'advance', 'happy', 'birthday', 'mama', ':', ')', ':', ')', ':', ')', '#', '77', '#', 'bless', '#', 'love', '#', 'bliss', 'https', ':', '//t.co/y16qtyyext', 'via', '@', 'youtube']]\n"
     ]
    }
   ],
   "source": [
    "emojiConv = ConvertEmojis(tokenizermodel)\n",
    "\n",
    "no_emoji = emojiConv.fit_transform(no_periods)\n",
    "\n",
    "print([no_emoji[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.) Break down compound words\n",
    "\n",
    "Replaces compound words such as \"strongwoman\", \"makelove\",  and \"followme\" (which likely are not inside the word embedding vocabulary yet probably contains valuable context information) into their simpler forms. In the case where there are multiple possible ways to break up a word, the `wordfreq` library's `word_frequency` method is used to get the frequency of each word (between 0.0 and 1.0) from a large corpus. The two frequencies are multiplied together, giving `both_word_freq` for each combination. The word combination with the highest `both_word _freq` is chosen to replace the compound word inside the tweet. Multiplying the two words in each combination also allows us to prioritize compound words that can stand as compound words. For example, we would rather keep \"mortage\" as it is instead of breaking it down into \"mort\" and \"gage.\" This should also hold true for compound words seperated by a \"-\":\n",
    "\n",
    "\"poetrycommunity\" -> \\[\"poetry\",\"community\"\\]\n",
    "\n",
    "\"best-selling\" -> \\[\"best-selling\"\\] \n",
    "\n",
    "\"deephouse\" -> \\[\"deep\",\"house\"\\]\n",
    "\n",
    "\"mortgage\" -> \\[\"mortgage\"\\]\n",
    "\n",
    "\"fight-club\" -> \\[\"fight\",\"club\"\\] (NOTE: NO EXAMPLE OF THIS FROM THE SAMPLE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "\n",
    "class BreakupWords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def break_compound_word(self,compound_word):\n",
    "        possible_words = []\n",
    "        first_word=\"\"\n",
    "        for i,xchar in enumerate(compound_word):\n",
    "            second_word = compound_word[i+1:]\n",
    "            if(xchar==\"-\"):\n",
    "                try:\n",
    "                    self.model[first_word]\n",
    "                    self.model[second_word]\n",
    "                    possible_words.append([first_word,second_word])\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            first_word+=xchar\n",
    "            try:          \n",
    "                self.model[first_word]\n",
    "                self.model[second_word]\n",
    "                possible_words.append([first_word,second_word])\n",
    "            except:\n",
    "                if(second_word==\"\"):\n",
    "                    try:\n",
    "                        self.model[first_word]\n",
    "                        possible_words.append([first_word])\n",
    "                    except:\n",
    "                        pass\n",
    "        return possible_words\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = copy.deepcopy(X)\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "            for ii, token in enumerate(tweet):\n",
    "                child_words = self.break_compound_word(token)\n",
    "                highestfrq = 0\n",
    "                if(len(child_words)!= 0):\n",
    "                    for child_word_set in child_words:\n",
    "                        if(len(child_word_set)!=1):\n",
    "                            both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                            if (both_word_freq > highestfrq):\n",
    "                                most_likely_combo = child_word_set\n",
    "                                highestfrq = both_word_freq\n",
    "                        else:\n",
    "                            most_likely_combo = child_word_set\n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + most_likely_combo + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(most_likely_combo)-1\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage of the feature extraction, the word embedding model was introduced since the `BreakupWords` class will needa vocabulary set to look for simple words in compound words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensimapi\n",
    "\n",
    "vectorizer_model = gensimapi.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', '@', 'suzialbracht', ':', 'the', 'ghost', 'fixer', 'https', ':', '//t.co/ngfdrzdlqt', 'a', 'freak', 'car', 'accident', 'stole', 'my', 'life', 'from', 'me', 'and', 'left', 'my', 'spirit', 'earthbound', 'sounds'], ['``', 'national', 'forecast', 'for', 'january', '5', ',', '2020', \"''\", 'via', 'fox', 'news', 'https', ':', '//t.co/xukxse0q6m', 'https', ':', '//t.co/conoqih2aa', '#', 'mlb', '#', 'baseball', '#', 'dfs', '#', 'love', '#', 'ny', '#', 'lineup', '#', 'softball', '#', 'dk', '#', 'fd', '#', 'usa', '#', 'homerun', '#', 'funny', '#', 'haha', '#', 'wtf', '#', '#', 'twins', '#', 'astros', '#', 'rangers', '#', 'redsox', '#', 'whitesox', '#', 'usa', '#', 'nba', '#', 'video', '#', 'money', '#', 'fantasy', '#', 'night'], ['do', \"n't\", 'get', 'stuck', 'with', 'a', 'bad', 'lender', '!', 'loan', 'with', 'me', 'and', 'see', 'just', 'how', 'easy', 'it', 'can', 'be', '!', 'https', ':', '//t.co/oshnvwe54k', '#', 'life', '#', 'love', '#', 'lockwithleslie', '#', 'home', '#', 'mortgages', '#', 'realestate', '#', 'realtor', '#', 'mortgage', '#', 'mortgage', 'broker', '#', 'loan', 'officer', '#', 'mortgage', 'lender', 'https', ':', '//t.co/dfypjxzrlz'], ['rt', '@', 'vclinebarton', ':', 'firework', 'sparkler', 'inspiring', 'words', 'to', 'ponder', 'this', 'first', '#', 'saturday', 'morning', 'of', 'the', '#', 'newyear', '2020', '!', 'grinning', 'face', 'with', 'smiling', 'eyes', 'shooting', 'star', '#', 'happy', 'saturday', 'friends', '!', 'sparkling', 'heart', 'crown', 'üïµÔ∏è\\u200d‚ôÇÔ∏è', 'cocktail', 'glass', '#', 'saturday'], ['wisdom', ':', 'if', 'you', 'find', 'yourself', 'with', 'an', 'empty', 'fridge', 'always', 'remember', 'burger', 'king', 'is', 'only', 'a', 'bus', 'ride', 'away', '#', 'workout', '#', 'love', '#', 'bot', 'life', 'https', ':', '//t.co/mpuc8j6sv8'], ['do', \"n't\", 'stop', 'find', 'someone', 'who', 'will', 'always', 'be', 'in', 'your', 'team', '-', 'left', 'pointing', 'magnifying', 'glass', '#', 'writing', '#', 'writer', '#', 'writersofinstagram', '#', 'poetry', '#', 'love', '#', 'writing', 'community', '#', 'quotes', '#', 'writers', '#', 'poem', '#', 'poet', '#', 'poetry', 'community', '#', 'writers', 'community', '#', 'poetsofinstagram', 'https', ':', '//t.co/3tqhxy2ri7'], ['#', 'now', 'playing', 'on', '@', 'glams_radio', ':', '#', 'night', 'tale', '(', 'original', 'mix', ')', 'by', '#', 'dr', 'space', '!', 'tune', 'in', 'now', 'and', 'enjoy', 'sound', 'of', '#', 'deep', 'house', '#', 'love', '#', 'music'], ['rt', '@', 'tebow', 'foundation', ':', 'our', 'best-selling', 'ttf', 'legacy', 'burnout', 'tee', 'is', 'back', 'in', 'stock', '!', 'enjoy', 'free', 'shipping', 'today', 'only', 'in', 'the', 'ttf', 'store', 'shop', 'to', 'sha'], ['rt', '@', 'itsmsshelly', ':', 'still', 'have', 'the', '#', 'flu', 'still', 'have', 'fever', 'still', 'feeling', 'yucky', 'but', ',', 'i', 'have', '#', 'happiness', ',', '#', 'inner', 'peace', ',', '&', 'amp', ';', 'more', '#', 'love', 'than', 'i', 'know'], ['mama', 'sings', ':', ')', 'advance', 'happy', 'birthday', 'mama', ':', ')', ':', ')', ':', ')', '#', '77', '#', 'bless', '#', 'love', '#', 'bliss', 'https', ':', '//t.co/y16qtyyext', 'via', '@', 'youtube']]\n"
     ]
    }
   ],
   "source": [
    "breakupWords = BreakupWords(vectorizer_model)\n",
    "\n",
    "simple_words = breakupWords.fit_transform(no_emoji)\n",
    "\n",
    "print([simple_words[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining texts show a lot of unnnecessary punctuations that do are not serving anything towards the meaning of the hashtag, which is why another feature extractor was created to remove all the given punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemovePunctuation(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,charsequence):\n",
    "        self.charsequence = set(charsequence)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for i,tweet in enumerate(X):\n",
    "            \n",
    "            X_copy[i] = [text for text in tweet if not set([text]).issubset(self.charsequence)]\n",
    "            \n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rt', 'suzialbracht', 'the', 'ghost', 'fixer', 'https', '//t.co/ngfdrzdlqt', 'a', 'freak', 'car', 'accident', 'stole', 'my', 'life', 'from', 'me', 'and', 'left', 'my', 'spirit', 'earthbound', 'sounds'], ['national', 'forecast', 'for', 'january', '5', '2020', 'via', 'fox', 'news', 'https', '//t.co/xukxse0q6m', 'https', '//t.co/conoqih2aa', 'mlb', 'baseball', 'dfs', 'love', 'ny', 'lineup', 'softball', 'dk', 'fd', 'usa', 'homerun', 'funny', 'haha', 'wtf', 'twins', 'astros', 'rangers', 'redsox', 'whitesox', 'usa', 'nba', 'video', 'money', 'fantasy', 'night'], ['do', \"n't\", 'get', 'stuck', 'with', 'a', 'bad', 'lender', '!', 'loan', 'with', 'me', 'and', 'see', 'just', 'how', 'easy', 'it', 'can', 'be', '!', 'https', '//t.co/oshnvwe54k', 'life', 'love', 'lockwithleslie', 'home', 'mortgages', 'realestate', 'realtor', 'mortgage', 'mortgage', 'broker', 'loan', 'officer', 'mortgage', 'lender', 'https', '//t.co/dfypjxzrlz'], ['rt', 'vclinebarton', 'firework', 'sparkler', 'inspiring', 'words', 'to', 'ponder', 'this', 'first', 'saturday', 'morning', 'of', 'the', 'newyear', '2020', '!', 'grinning', 'face', 'with', 'smiling', 'eyes', 'shooting', 'star', 'happy', 'saturday', 'friends', '!', 'sparkling', 'heart', 'crown', 'üïµÔ∏è\\u200d‚ôÇÔ∏è', 'cocktail', 'glass', 'saturday'], ['wisdom', 'if', 'you', 'find', 'yourself', 'with', 'an', 'empty', 'fridge', 'always', 'remember', 'burger', 'king', 'is', 'only', 'a', 'bus', 'ride', 'away', 'workout', 'love', 'bot', 'life', 'https', '//t.co/mpuc8j6sv8'], ['do', \"n't\", 'stop', 'find', 'someone', 'who', 'will', 'always', 'be', 'in', 'your', 'team', '-', 'left', 'pointing', 'magnifying', 'glass', 'writing', 'writer', 'writersofinstagram', 'poetry', 'love', 'writing', 'community', 'quotes', 'writers', 'poem', 'poet', 'poetry', 'community', 'writers', 'community', 'poetsofinstagram', 'https', '//t.co/3tqhxy2ri7'], ['now', 'playing', 'on', 'glams_radio', 'night', 'tale', 'original', 'mix', 'by', 'dr', 'space', '!', 'tune', 'in', 'now', 'and', 'enjoy', 'sound', 'of', 'deep', 'house', 'love', 'music'], ['rt', 'tebow', 'foundation', 'our', 'best-selling', 'ttf', 'legacy', 'burnout', 'tee', 'is', 'back', 'in', 'stock', '!', 'enjoy', 'free', 'shipping', 'today', 'only', 'in', 'the', 'ttf', 'store', 'shop', 'to', 'sha'], ['rt', 'itsmsshelly', 'still', 'have', 'the', 'flu', 'still', 'have', 'fever', 'still', 'feeling', 'yucky', 'but', 'i', 'have', 'happiness', 'inner', 'peace', 'amp', 'more', 'love', 'than', 'i', 'know'], ['mama', 'sings', 'advance', 'happy', 'birthday', 'mama', '77', 'bless', 'love', 'bliss', 'https', '//t.co/y16qtyyext', 'via', 'youtube']]\n"
     ]
    }
   ],
   "source": [
    "noPunctuation = RemovePunctuation([\".\",\"#\",\":\",\"‚Ä¢\",\",\",\"@\",\"\\\"\",\";\",\"\\'\",\")\",\"(\",\"&\",\"``\",\"\\'\\'\"])\n",
    "\n",
    "no_punc = noPunctuation.fit_transform(simple_words)\n",
    "\n",
    "print([no_punc[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text looks a lot cleaner now with more 'vectorizable' words. Now we can finally convert all the text into vectors.\n",
    "\n",
    "Now we have a list of vectors for each token which which was successfully converted. Our final step should be modifying the dimensions of the list into one which can be easily used in scikit-learn classifiers. We have three options:\n",
    "\n",
    "\n",
    "#### 1.) Flatten out all the vectors for a given tweet into a 1 dimensional list of numbers.\n",
    "\n",
    "#####    Pro: \n",
    "\n",
    "        Preserves all the vectors\n",
    "    \n",
    "#####    Con: \n",
    "    \n",
    "        Will have too many dimensions (Current maximum twitter post character limit is 280. If a tweet were to go something like \"i i i i i...\" with i repeating 140 times, there will be 50\\*140=7000 dimensions!) and will signifantly affect classifer performance\n",
    "    \n",
    "        Order of the words in a tweet should not matter. In other words, there should not be any inherent difference between the tweet \"love is important\" and \"is important love.\" We are only looking at the relationship between the words in the tweet and the hashtags\n",
    "    \n",
    "#### 2.) Sum up all the vectors in a tweet.\n",
    "\n",
    "#####    Pro:\n",
    "        \n",
    "        No \"Curse of Dimensionality Problem\"\n",
    "        \n",
    "        Does not take order of the words into account\n",
    "        \n",
    "#####    Con:\n",
    "    \n",
    "        A lot more variation within the data points\n",
    "        \n",
    "#### 3.) Take an average of all the vectors in a tweet\n",
    "\n",
    "#####    Pro:\n",
    "    \n",
    "        No \"Curse of Dimensionality Problem\"\n",
    "        \n",
    "        Does not take order of the words into account\n",
    "        \n",
    "        Does not cause much variation within the data points\n",
    "        \n",
    "#####    Con:\n",
    "    \n",
    "        If all the vectors of of words in a tweet are far from each other (Ex: On a 3dimensional space if all the data points form a sphere), the average will be some point that has close to no correlation with each points in the space\n",
    "        \n",
    "\n",
    "This project used the second choice, but in future versions, all choices 1 and 3 should also be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizeTweets(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_vectorized = []\n",
    "        for tweet in X:\n",
    "            tweet_vectorized = []\n",
    "            for token in tweet:\n",
    "                try:\n",
    "                    vector = self.model[token]\n",
    "                    tweet_vectorized.append(vector)\n",
    "                except:\n",
    "                    pass\n",
    "            X_vectorized.append(tweet_vectorized)\n",
    "        \n",
    "        return X_vectorized\n",
    "    \n",
    "class SumUpTweetVector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        tweet_vector_sums = []\n",
    "        vector_shape = len(X[0][0])\n",
    "        for tweet in X:\n",
    "            vector_sum = np.zeros(vector_shape)\n",
    "            for vector in tweet:\n",
    "                vector_sum += vector\n",
    "                \n",
    "            tweet_vector_sums.append(vector_sum)\n",
    "            \n",
    "        return np.array(tweet_vector_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  4.95226901,   3.15686597,   0.25663711,  -3.78946606,\n",
      "         5.72923988,   2.64818394,  -6.50811001,  -0.31704699,\n",
      "        -1.11409814,   1.82877156,  -1.47907597,   4.12991197,\n",
      "        -7.04476113,   1.51743902,   5.79381247,  -1.925828  ,\n",
      "        -3.33800095,   5.70873202,  -6.01290099,  -5.46673896,\n",
      "        -2.53265295,   9.43682811,   0.49462399,   2.48373532,\n",
      "         8.48723485, -23.39799008, -11.50549011,   5.11734487,\n",
      "        12.19873604,  -5.41126891,  43.92079987,   1.64401296,\n",
      "        -4.91623806,   0.47500407,  -0.06193791,   4.33825792,\n",
      "         2.72299994,  -4.22834764,   4.21625101,  -2.25497804,\n",
      "         0.68742408,   0.51830799,  -4.381761  ,   3.12400904,\n",
      "         0.77917704,  -0.71636501,   0.60973888,  -7.45060703,\n",
      "         5.13054807,  -1.60230596]), array([-11.07284827,  14.35666701,   1.63315413,  18.54431606,\n",
      "        -7.24858415,  -8.64160295, -15.94223228, -10.08173362,\n",
      "        -4.63276496,   1.47072607,  -1.74792978,   3.55550306,\n",
      "       -11.12603487,   8.3347081 ,  17.741391  ,  -1.60283711,\n",
      "        -7.14554803,   1.41246117, -16.80560888,   7.91732707,\n",
      "        -4.79382486,  -4.78851279,   9.12208491,   8.19180702,\n",
      "        -9.59336501, -19.40670674,  -3.64859594,  -1.53494607,\n",
      "         5.06717658, -14.94931586,  52.77252001,  11.16106902,\n",
      "        -4.45232613,   4.90802549,  -7.24419213,  -3.6550187 ,\n",
      "         7.249928  ,  -6.52386202,  -4.60583067, -10.06047809,\n",
      "         2.41267901,  -5.6135504 ,  -2.19476786,   3.0600151 ,\n",
      "        -6.1062751 ,   6.71629197,   2.43215396,   2.56875408,\n",
      "        -5.79555101,  12.45887108]), array([ 11.1679931 ,   3.74291894,   8.99244302,  -4.28237803,\n",
      "         5.83098777,   0.3197085 , -16.13239299,  -0.55991274,\n",
      "        -9.93389451,   9.67527786,  -6.80012681,  18.13036564,\n",
      "        -9.35340304,  -2.51107596,  18.48494606,  10.70073878,\n",
      "         2.54301996,  -0.14389263,   8.00244321,  -5.82142294,\n",
      "         6.77360019,   5.51436709,  -1.51920487,   7.86259101,\n",
      "        -0.43913034, -44.91530962,  -7.83445607,   2.90107766,\n",
      "        17.07555449, -14.52044898,  95.90376979,   7.96214595,\n",
      "         0.47595074,   5.38774421,   2.58107492,   0.94642899,\n",
      "        -2.87154258,  -7.76039682,  17.63446022, -25.44489592,\n",
      "        -1.50225268,  -2.51383794,   5.70295629,  13.77997384,\n",
      "        -1.4981481 ,  -3.69779135,   0.60028193,   3.67526138,\n",
      "        11.5887911 ,   8.57988614]), array([  1.13582994,  12.77093193,  -3.76336087,   0.15670398,\n",
      "        12.56888093,  -4.07658511,  -7.00469039,  -1.0649281 ,\n",
      "        -7.74411229,   4.97239694,  -6.8801281 ,  -7.00484743,\n",
      "        -6.84516967,   6.27434091,  13.75602571,   0.84775784,\n",
      "        -6.12011911,   0.3347888 , -17.61839197, -11.91333599,\n",
      "        -1.16857504,  10.83929981,   5.56889936,  -0.92663693,\n",
      "         6.45305484, -35.55735988, -11.661164  ,  13.21118902,\n",
      "         6.24085363,  -9.53699108,  67.35459444,  13.25735983,\n",
      "        -6.97313015,   0.582082  ,  -3.30342243,   0.32320731,\n",
      "         7.83819324,   0.55573419,  -2.36718509,  -6.71281996,\n",
      "        -0.45898056,   6.11507803,  -6.57297008,  -2.42304531,\n",
      "        10.25892424,   7.39819102,  -2.31468405,  -9.44491812,\n",
      "         3.0010026 ,  -3.36768148]), array([  8.92083804,   5.89619993,  -1.67735395,  -5.1898831 ,\n",
      "        12.3102399 ,  -0.5161551 ,  -6.95576199,  -0.07071935,\n",
      "        -0.25465708,   3.11304448,  -4.50381399,   6.91903999,\n",
      "        -6.57003803,   1.65325549,  11.84535891,   6.41765796,\n",
      "         3.06901894,   4.93086602,  -2.73090591, -10.16627887,\n",
      "        -0.17994195,   8.01733335,   0.74154808,   6.98918494,\n",
      "         9.26750597, -31.27684028,  -9.19210596,   5.95112019,\n",
      "        15.38642521,  -8.13482694,  60.05422577,   7.73409908,\n",
      "        -8.80990006,   3.2258324 ,   0.70021699,   4.12586098,\n",
      "         4.61156999,   2.90792001,   1.53815711,  -4.73523397,\n",
      "         3.00830502,   3.4356137 ,  -5.11511993,   8.55916038,\n",
      "        -0.16596296,   2.78638696,   0.33778405,  -7.142396  ,\n",
      "         2.00381904,   3.05731584]), array([ -1.68712105,  15.86527728,  -7.39704595,  -9.72403821,\n",
      "        11.626026  ,   1.58611609, -15.82504702,  -2.66528529,\n",
      "        -8.29784704,   5.89139089,  -7.89751202,   8.83024196,\n",
      "        -5.87158311,  -0.87203654,  14.7738562 ,  -3.71137309,\n",
      "         6.95436886,  -4.39858786,   3.61629701,  -7.64923815,\n",
      "         3.45773859,  14.82838499,   5.49764861,  11.98367466,\n",
      "        17.51029181, -44.29430014, -19.867596  ,  -4.54153692,\n",
      "         2.17916327, -13.05293092,  84.65302075,  -0.69210544,\n",
      "        -5.5137081 , -13.63668199,  -7.47600645,   3.23820608,\n",
      "        -3.94577487,   5.0786485 ,   1.31337011,  -1.88863797,\n",
      "        10.17410502,  10.13463763,  -2.53657939,   3.15698911,\n",
      "         2.36871011,   4.91707406,  -1.14405488,  -2.25102958,\n",
      "        -4.12821307,   0.35900599]), array([ 2.55064981e+00,  7.75609098e+00, -6.03210992e+00, -1.69411200e+00,\n",
      "        4.26705689e+00,  3.13557205e+00, -1.18796042e+01, -4.11937365e+00,\n",
      "       -6.37237600e+00,  5.30010501e+00, -2.79852104e+00,  3.40972605e+00,\n",
      "       -4.57691610e+00,  2.21268756e+00,  6.50023110e+00,  1.05110755e+00,\n",
      "        2.59238449e+00,  3.60525101e+00, -8.77226091e+00, -6.40962113e+00,\n",
      "        4.92520599e+00,  9.65519399e+00,  4.36587399e+00,  2.42287139e+00,\n",
      "        6.57318499e+00, -2.52494792e+01, -1.39307610e+01,  2.35848574e+00,\n",
      "        5.19107515e+00, -6.65021691e+00,  6.77642996e+01,  6.03239931e-01,\n",
      "       -2.01053302e+00, -3.81640106e+00, -6.70613239e-01,  1.67110336e+00,\n",
      "        1.24373893e+00, -1.65491795e+00,  5.30883286e-03, -4.96546889e+00,\n",
      "       -3.55574094e-01,  2.57706385e+00, -4.45559101e+00, -3.12937806e+00,\n",
      "       -1.94820211e+00,  7.84845390e+00, -1.56078396e+00, -1.04418129e+01,\n",
      "       -5.63373203e+00,  8.32099944e-01]), array([ -1.89350778,   5.80766291,   0.98286407,   1.33088499,\n",
      "         5.0224667 ,  -6.89465115, -10.98917602, -10.04215082,\n",
      "         2.85593976,   1.06752147,   1.31335499,   2.53783893,\n",
      "        -7.91507788,  -2.638086  ,   6.59736859,   5.66705033,\n",
      "        -0.35630395,   1.78428784,  -2.74538492,  -3.03022798,\n",
      "         3.95245011,  -4.29548198,  -1.18979267,   3.88661417,\n",
      "        -1.17907622, -26.40116709, -10.99429406,   0.75336603,\n",
      "         4.73056302,  -8.96110607,  57.64941128,   5.61573583,\n",
      "        -2.72026776,   5.06327317,   1.14597522,  -3.67740616,\n",
      "         3.50042195,   1.10290707,   2.07638555,   0.45659887,\n",
      "        -2.56934806,  -1.99536236,  -1.95881406,   2.97468974,\n",
      "         0.68553222,  -0.10765407,   0.58774473,  -6.19470518,\n",
      "         3.20327201,   2.09797299]), array([  9.0211549 ,   2.70097888,   0.91684807,  -5.47462796,\n",
      "         7.32485872,   2.94449003,  -6.53556106,  -1.68846357,\n",
      "        -3.66940628,   3.0062271 ,  -0.21826192,   2.31262398,\n",
      "        -8.04029197,  -5.34810099,  12.03958048,   6.12551803,\n",
      "         0.165971  ,   1.36244021,  -2.58442496,  -9.87856384,\n",
      "       -10.00323988,   8.91979413,   9.85600796,   0.29217678,\n",
      "         9.121314  , -31.78596027, -13.67933008,   4.46022704,\n",
      "        10.59021027,  -5.62740799,  67.03395966,  12.05106786,\n",
      "        -0.48177001,  -5.40039413,  -2.86399885,  -4.17158718,\n",
      "        -1.11678802,  -1.14540163,   3.26692159,  -3.87069937,\n",
      "        -7.70817062,   0.91593312,   0.74141597,   7.62607502,\n",
      "         4.49385099,   5.901852  ,  -5.0803211 ,  -2.71277293,\n",
      "        -5.02398024,   1.47240796]), array([ 0.92784813,  4.68073284, -1.70178005, -0.70873004,  3.12868396,\n",
      "       -1.82334203, -3.14337993,  1.34433643,  0.94433901,  6.31691295,\n",
      "       -1.43553902,  2.84064001,  0.50475197, -2.31852398,  5.01376702,\n",
      "       -2.11789999, -3.92004792, -0.55253999,  1.39187706,  1.48284901,\n",
      "        0.4975    ,  6.81017002,  5.06950405,  4.45506427,  6.98719592,\n",
      "       -6.89910392, -5.69863912, -0.98220003,  4.80416206, -9.46009312,\n",
      "       19.94876973,  4.19726993, -4.08661407,  4.21784203, -3.16325294,\n",
      "       -2.49216101,  4.38436005, -2.93610001, -2.44624998, -4.09432995,\n",
      "        3.62031103, -2.27316003, -4.08625981, -4.99385999, -0.63637001,\n",
      "        0.819113  , -0.26545504, -9.86696023, -1.27383991,  4.47311797])]\n"
     ]
    }
   ],
   "source": [
    "tweetVectorizer = VectorizeTweets(vectorizer_model)\n",
    "\n",
    "tweets_v = tweetVectorizer.fit_transform(no_punc)\n",
    "\n",
    "vectorSumUp = SumUpTweetVector()\n",
    "\n",
    "tweets_v_sum = vectorSumUp.fit_transform(tweets_v)\n",
    "\n",
    "print([tweets_v_sum[i] for i in perm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future TODO:\n",
    "\n",
    " 1. Create dataset that utilize option 1 (from combining tweet-word vectors)\n",
    " 2. Create dataset that utilize option 3 (from combining tweet-word vectors)\n",
    " 3. Recognize combined words comprised of 3 or more simpler words (Ex: \"photooftheday\" -> \"photo\" \"of \" \"the\" \"day\")\n",
    " 4. Create dataset that calculates TF-IDF of all the words and uses 1 key word from each tweet to represent the tweet\n",
    " 5. Create dataset that calculates TF-IDF of all the words and uses 1 key word and multiplies the vector for each word with its respective tf-idf value, and then takes an average of all the vectors in a tweet\n",
    " 6. Implement Doc2Vec to represent each tweet instead of trying to combine \n",
    " 7. UPDATE - NLTK has a Twitter Tokenizer that preserves text-emojis such as :) and :-). Create a dataset using this tokenizer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
