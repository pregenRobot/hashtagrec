{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-28T15:13:57.208Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensimapi\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "import unicodedata as ud\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import numpy as np\n",
    "!pip install wordfreq\n",
    "from wordfreq import word_frequency\n",
    "\n",
    "consumer_key = \"idjkP1aobw1UQd8xZ9RYiY5CZ\"\n",
    "consumer_secret = \"jZFXsLJRtvR4pQvmuTJ94mnr1TJ0tYz1w4s0XI5TpR4U5tEnXe\"\n",
    "access_token = \"1001251273981677568-5SxiGu3SisqPnzY3Zkq8QHh7vreYar\"\n",
    "access_token_secret = \"XZn1rvLw10JnxJgKx05sW4eN0HqhaVjsasaqV5tEytsTu\"\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:09:32.793463Z",
     "start_time": "2019-12-28T15:09:14.294190Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TweepError",
     "evalue": "Failed to send request: Could not find a suitable TLS CA certificate bundle, invalid path: /usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/certifi/cacert.pem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mcert_verify\u001b[0;34m(self, conn, url, verify, cert)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Could not find a suitable TLS CA certificate bundle, invalid path: /usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/certifi/cacert.pem",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-2bca01b75abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpossible_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"#love\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgeocode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"51.5098,-0.1180,1km\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"extended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtweet_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtweet_text_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtbwTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36mcert_verify\u001b[0;34m(self, conn, url, verify, cert)\u001b[0m\n",
      "\u001b[0;31mTweepError\u001b[0m: Failed to send request: Could not find a suitable TLS CA certificate bundle, invalid path: /usr/local/anaconda3/envs/hashtagrec/lib/python3.7/site-packages/certifi/cacert.pem"
     ]
    }
   ],
   "source": [
    "tweetnum = 100\n",
    "\n",
    "tbwTokenizer = TreebankWordTokenizer()\n",
    "glove50Model = gensimapi.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "tokenizedText = []\n",
    "tknTextVector = []\n",
    "\n",
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "\n",
    "def break_compound_word(compound_word,model):\n",
    "    first_word = \"\"\n",
    "    possible_words = []\n",
    "    for i,xchar in enumerate(compound_word):\n",
    "        first_word+=xchar\n",
    "        try:\n",
    "            model[first_word]\n",
    "            second_word = compound_word[i+1:]\n",
    "            \n",
    "            print(first_word)\n",
    "            print(second_word)\n",
    "            print(\"---\")\n",
    "            model[second_word]\n",
    "            possible_words.append([first_word,second_word])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return possible_words\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#love\",geocode=\"51.5098,-0.1180,1km\",tweet_mode=\"extended\").items(tweetnum):\n",
    "    tweet_text = tweet._json[\"full_text\"].lower()\n",
    "    tweet_text_tokenized = tbwTokenizer.tokenize(tweet_text)\n",
    "    tokenizedText.append(tweet_text_tokenized)\n",
    "    tknVectorList = []\n",
    "    ##Deal with emojis\n",
    "    ##Deal with capital letters - ok\n",
    "    ##Access the link, and if there is text, look for keywords using tf-idf\n",
    "    ##Auto correct\n",
    "    \n",
    "    for text in tweet_text_tokenized:\n",
    "        try:\n",
    "            wordVector = glove50Model[text]\n",
    "            tknVectorList.append(wordVector)\n",
    "        except:\n",
    "            solutionFound = False\n",
    "            if(text[-1] == \".\"):\n",
    "                try:\n",
    "                    wordVector = glove50Model[text[:-1]]\n",
    "                    tknVectorList.append(wordVector)\n",
    "                    solutionFound = True\n",
    "                except:\n",
    "                    pass\n",
    "            if(has_emoji(text)):\n",
    "                em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "                em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                for eachEmoji in em_split:\n",
    "                    emojiDetail = ud.name(eachEmoji)\n",
    "                    emojiTokenized = tbwTokenizer.tokenize(emojiDetail.lower())\n",
    "                    emojiWordVectors = []  \n",
    "                    for descr in emojiTokenized:\n",
    "                        try:\n",
    "                            emojiVector = glove50Model[descr]\n",
    "                            emojiWordVectors.append(emojiVector)\n",
    "                        except:\n",
    "                            pass\n",
    "                    tknVectorList.extend(emojiWordVectors)\n",
    "                solutionFound = True\n",
    "                \n",
    "            child_words = break_compound_word(text,glove50Model)\n",
    "            highestfrq = 0\n",
    "            if(len(child_words)!= 0):\n",
    "                for child_word_set in child_words:\n",
    "                    both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                    if (both_word_freq > highestfrq):\n",
    "                        most_likely_combo = child_word_set\n",
    "                        highestfrq = both_word_freq\n",
    "            \n",
    "                child_word_vectors = []\n",
    "                child_word_vectors.append(glove50Model[most_likely_combo[0]])\n",
    "                child_word_vectors.append(glove50Model[most_likely_combo[1]])\n",
    "                tknTextVectorList.extend(child_word_vectors)\n",
    "                solutionFound = True\n",
    "            \n",
    "            if(not solutionFound):\n",
    "                print(\"Failed to vectorize: \" + text)\n",
    "                print(\"    \" + str(has_emoji(text)))\n",
    "                    \n",
    "    tknTextVector.append(tknVectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T08:05:57.770034Z",
     "start_time": "2019-12-28T08:05:57.748592Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknTextVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T09:21:30.825125Z",
     "start_time": "2019-12-28T09:21:30.819234Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import unicodedata as ud\n",
    "import emoji\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "\n",
    "s = '\\U0001F604\\U0001F600\\U0001F608'\n",
    "# for c in s:\n",
    "#     print('{} U+{:5X} {}'.format(c,ord(c),ud.name(c)))\n",
    "    \n",
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T09:21:54.468875Z",
     "start_time": "2019-12-28T09:21:54.462346Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testStringS = \"hello🥰🥰🥰🥰🥰🥰\"\n",
    "len(testStringS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T09:11:04.646178Z",
     "start_time": "2019-12-28T09:11:04.640880Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🥰'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testStringS[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T09:21:57.654754Z",
     "start_time": "2019-12-28T09:21:57.648798Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean = has_emoji(testStringS)\n",
    "\n",
    "boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T10:16:48.745225Z",
     "start_time": "2019-12-28T10:16:48.675021Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-ad2d5fbb2f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreduce_lengthening\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(.)\\1{2,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pattern'"
     ]
    }
   ],
   "source": [
    "from pattern.en import spelling\n",
    "\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "\n",
    "word = \"amazzziiing\"\n",
    "word_wlf = reduce_lengthening(word) #calling function defined above\n",
    "print(word_wlf) #word lengthening isn't being able to fix it completely\n",
    "\n",
    "correct_word = pattern.en.pattenspelling(word_wlf) \n",
    "print(correct_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T11:14:02.963517Z",
     "start_time": "2019-12-28T11:14:02.954866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'doggy']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "possible_words = break_compound_word(\"fearfull\",glove50Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T14:32:40.773798Z",
     "start_time": "2019-12-28T14:32:40.762410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "hedog\n",
      "---\n",
      "th\n",
      "edog\n",
      "---\n",
      "the\n",
      "dog\n",
      "---\n",
      "thed\n",
      "og\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.2807999e-01, -1.3812999e-01, -9.8856997e-01, -1.5544000e-01,\n",
       "        1.0504800e+00,  4.9548301e-01, -1.5754800e+00, -5.8007997e-01,\n",
       "        1.1497399e+00, -1.2244000e+00,  2.8232771e-01,  3.8110998e-01,\n",
       "        8.8839948e-02,  6.1919999e-01,  4.7598049e-01, -1.7241201e-01,\n",
       "        2.8213999e-01,  7.8604996e-01, -2.0422001e+00, -6.9960999e-01,\n",
       "       -3.9665100e-01,  3.2700002e-03,  6.1680001e-01,  5.2424002e-01,\n",
       "        2.6324999e-01, -3.6494000e+00, -1.8177600e+00,  5.2471101e-01,\n",
       "       -2.3199916e-03, -8.7852997e-01,  5.5743999e+00,  9.0909988e-02,\n",
       "       -1.1399500e+00,  3.2957000e-01, -7.6403871e-02,  3.7862489e-01,\n",
       "        3.0857998e-01, -6.1034000e-01,  2.6602101e-01, -7.9814303e-01,\n",
       "       -3.8490897e-01,  8.3189994e-02, -9.9576998e-01,  7.8985298e-01,\n",
       "        8.0769002e-01, -3.2594001e-01,  4.5024902e-02, -1.0652900e+00,\n",
       "        6.0065997e-01, -4.0061998e-01], dtype=float32)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def break_compound_word(compound_word,model):\n",
    "    first_word = \"\"\n",
    "    possible_words = []\n",
    "    for i,xchar in enumerate(compound_word):\n",
    "        first_word+=xchar\n",
    "        try:\n",
    "            model[first_word]\n",
    "            second_word = compound_word[i+1:]\n",
    "            \n",
    "            print(first_word)\n",
    "            print(second_word)\n",
    "            print(\"---\")\n",
    "            model[second_word]\n",
    "            possible_words.append([first_word,second_word])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return possible_words\n",
    "\n",
    "wordset = break_compound_word(\"thedog\",glove50Model)\n",
    "\n",
    "sum_meaning = glove50Model[wordset[0][0]] + glove50Model[wordset[0][1]]\n",
    "\n",
    "sum_meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T14:29:48.961774Z",
     "start_time": "2019-12-28T14:29:48.954979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T14:41:45.183684Z",
     "start_time": "2019-12-28T14:41:45.166762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gibralter', 0.7775185108184814),\n",
       " ('scragg', 0.7496911287307739),\n",
       " ('aptx', 0.7440819144248962),\n",
       " ('kamya', 0.724817156791687),\n",
       " ('mcnicol', 0.7228055596351624),\n",
       " ('clac', 0.7217633724212646),\n",
       " ('wildlings', 0.718341588973999),\n",
       " ('eniola', 0.7131443023681641),\n",
       " ('blout', 0.7082638740539551),\n",
       " ('sidhwa', 0.7071182727813721)]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
