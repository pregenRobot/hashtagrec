{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:42:54.807188Z",
     "start_time": "2019-12-28T15:42:52.725981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordfreq in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (2.2.1)\n",
      "Requirement already satisfied: langcodes>=1.4.1 in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (1.4.1)\n",
      "Requirement already satisfied: regex<=2018.02.21,>=2017.07.11 in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (2018.2.21)\n",
      "Requirement already satisfied: msgpack in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from wordfreq) (0.6.2)\n",
      "Requirement already satisfied: marisa-trie in /usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages (from langcodes>=1.4.1->wordfreq) (0.7.5)\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gensimapi\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "import unicodedata as ud\n",
    "import functools\n",
    "import operator\n",
    "import re\n",
    "import numpy as np\n",
    "!pip install wordfreq\n",
    "from wordfreq import word_frequency\n",
    "import emoji\n",
    "\n",
    "consumer_key = \"idjkP1aobw1UQd8xZ9RYiY5CZ\"\n",
    "consumer_secret = \"jZFXsLJRtvR4pQvmuTJ94mnr1TJ0tYz1w4s0XI5TpR4U5tEnXe\"\n",
    "access_token = \"1001251273981677568-5SxiGu3SisqPnzY3Zkq8QHh7vreYar\"\n",
    "access_token_secret = \"XZn1rvLw10JnxJgKx05sW4eN0HqhaVjsasaqV5tEytsTu\"\n",
    "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:41:45.395304Z",
     "start_time": "2019-12-29T07:41:19.287332Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to vectorize: louieandlola\n",
      "    False\n",
      "Failed to vectorize: //t.co/ieevpvgkyf\n",
      "    False\n",
      "Failed to vectorize: //t.co/io5e5rveh4\n",
      "    False\n",
      "Failed to vectorize: teddybeardress\n",
      "    False\n",
      "Failed to vectorize: oversizedjumperdress\n",
      "    False\n",
      "Failed to vectorize: yeezyboost\n",
      "    False\n",
      "Failed to vectorize: //t.co/cij1kz1sjz\n",
      "    False\n",
      "Failed to vectorize: //t.co/kknpl6fuva\n",
      "    False\n",
      "Failed to vectorize: couplegoals‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/cosbbs0vgg\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/mcbk19f8py\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/bnmgfynker\n",
      "    False\n",
      "Failed to vectorize: //t.co/s2fwrjd6dt\n",
      "    False\n",
      "Failed to vectorize: blackandwhite\n",
      "    False\n",
      "Failed to vectorize: motheranddaughter\n",
      "    False\n",
      "Failed to vectorize: //t.co/gr7c42pjog\n",
      "    False\n",
      "Failed to vectorize: //t.co/nto6fxtso6\n",
      "    False\n",
      "Failed to vectorize: selfietime\n",
      "    False\n",
      "Failed to vectorize: drummondcomunic\n",
      "    False\n",
      "Failed to vectorize: //t.co/do9qojfazp\n",
      "    False\n",
      "Failed to vectorize: stillinthemoodoffestiveseason\n",
      "    False\n",
      "Failed to vectorize: //t.co/6qug8fhqxh\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/vs5tfxsalx\n",
      "    False\n",
      "Failed to vectorize: inlove‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/ccd1llgae3\n",
      "    False\n",
      "Failed to vectorize: menstreetstyle\n",
      "    False\n",
      "Failed to vectorize: //t.co/yqx8nlg7sz\n",
      "    False\n",
      "Failed to vectorize: //t.co/gknbfyu51w\n",
      "    False\n",
      "Failed to vectorize: //t.co/2z8e2usp7p\n",
      "    False\n",
      "Failed to vectorize: gayboy‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/eurcpltcgx\n",
      "    False\n",
      "Failed to vectorize: //t.co/5nc63ype6g\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: followyourpassion\n",
      "    False\n",
      "Failed to vectorize: //t.co/0s0begp5ws\n",
      "    False\n",
      "Failed to vectorize: ama_may_77\n",
      "    False\n",
      "Failed to vectorize: thankyouforbeingafriend\n",
      "    False\n",
      "Failed to vectorize: //t.co/qhqr6mt3x0\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: instafashion‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/xf1qrcwdtj\n",
      "    False\n",
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/ktt3tfgcso\n",
      "    False\n",
      "Failed to vectorize: re-strategise.\n",
      "    False\n",
      "Failed to vectorize: familylove‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/myc2dqn6dh\n",
      "    False\n",
      "Failed to vectorize: //t.co/mcwngkyypv\n",
      "    False\n",
      "Failed to vectorize: //t.co/xzujbpx8qh\n",
      "    False\n",
      "Failed to vectorize: //t.co/kyg5pqijbu\n",
      "    False\n",
      "Failed to vectorize: //t.co/rhlyrugoqk\n",
      "    False\n",
      "Failed to vectorize: //t.co/3ydznftkoy\n",
      "    False\n",
      "Failed to vectorize: //t.co/08mvy9uahq\n",
      "    False\n",
      "Failed to vectorize: //t.co/qwr5hjrbzt\n",
      "    False\n",
      "Failed to vectorize: //t.co/lurzb6yjyg\n",
      "    False\n",
      "Failed to vectorize: thisislondon\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: loveyou‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/mvdycvlzi8\n",
      "    False\n",
      "Failed to vectorize: felicit√†\n",
      "    False\n",
      "Failed to vectorize: //t.co/uffeagcmg8\n",
      "    False\n",
      "Failed to vectorize: //t.co/ftipd2t3m9\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: like4like\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/klovkqq2zi\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: //t.co/mof6p5fd5k\n",
      "    False\n",
      "Failed to vectorize: thanksforallthelove\n",
      "    False\n",
      "Failed to vectorize: peaceonearth\n",
      "    False\n",
      "Failed to vectorize: //t.co/g8sq93gxg9\n",
      "    False\n",
      "Failed to vectorize: //t.co/fbpsoi8yga\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: xmasishere\n",
      "    False\n",
      "Failed to vectorize: christmasishere\n",
      "    False\n",
      "Failed to vectorize: xmastree‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/9ck0bxor0s\n",
      "    False\n",
      "Failed to vectorize: followformore\n",
      "    False\n",
      "Failed to vectorize: mosocaphotography\n",
      "    False\n",
      "Failed to vectorize: mosocaphotos\n",
      "    False\n",
      "Failed to vectorize: //t.co/vmfbaes9jo\n",
      "    False\n",
      "Failed to vectorize: //t.co/mvcxyi1pgy\n",
      "    False\n",
      "Failed to vectorize: //t.co/zb0pqo4mke\n",
      "    False\n",
      "Failed to vectorize: //t.co/evsnhfysnq\n",
      "    False\n",
      "Failed to vectorize: //t.co/qmehrwmjfi\n",
      "    False\n",
      "Failed to vectorize: womeninbusiness\n",
      "    False\n",
      "Failed to vectorize: //t.co/vouasbzjnr\n",
      "    False\n",
      "Failed to vectorize: ‚Ä¢\n",
      "    False\n",
      "Failed to vectorize: rubywoolipstick\n",
      "    False\n",
      "Failed to vectorize: loveyourselffirst\n",
      "    False\n",
      "Failed to vectorize: //t.co/arv7ggly7b\n",
      "    False\n",
      "Failed to vectorize: ùìíùì±ùìªùì≤ùìºùìΩùì∂ùì™ùìº\n",
      "    False\n",
      "Failed to vectorize: ùìÆùìøùìÆùìªùîÇùì∏ùì∑ùìÆ..\n",
      "    False\n",
      "Failed to vectorize: christmasinlondon\n",
      "    False\n",
      "Failed to vectorize: wishestoall\n",
      "    False\n",
      "Failed to vectorize: //t.co/vdyfmslnpu\n",
      "    False\n",
      "Failed to vectorize: lovemyjob\n",
      "    False\n",
      "Failed to vectorize: //t.co/eembujqqdu\n",
      "    False\n",
      "Failed to vectorize: //t.co/xmpkgvdnq6\n",
      "    False\n",
      "Failed to vectorize: flyoncoldplay\n",
      "    False\n",
      "Failed to vectorize: peaceofmind\n",
      "    False\n",
      "Failed to vectorize: gk_tony\n",
      "    False\n",
      "Failed to vectorize: //t.co/pjumaluniv\n",
      "    False\n",
      "Failed to vectorize: //t.co/zwzlzuvjmc\n",
      "    False\n",
      "Failed to vectorize: encuentres\n",
      "    False\n",
      "Failed to vectorize: //t.co/f12gaps6ox\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: xmasishere‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/aetlpzvewb\n",
      "    False\n",
      "Failed to vectorize: cutemen‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/mwsafzjbgt\n",
      "    False\n",
      "Failed to vectorize: hawesandcurtis\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/b4xmrrrqvi\n",
      "    False\n",
      "Failed to vectorize: //t.co/lp838a0xtx\n",
      "    False\n",
      "Failed to vectorize: //t.co/32fm8yeyji\n",
      "    False\n",
      "Failed to vectorize: //t.co/3ojqsdhxzl\n",
      "    False\n",
      "Failed to vectorize: lanasiberie\n",
      "    False\n",
      "Failed to vectorize: //t.co/noohzaa1as\n",
      "    False\n",
      "Failed to vectorize: healthandhappiness\n",
      "    False\n",
      "Failed to vectorize: toastiefest\n",
      "    False\n",
      "Failed to vectorize: glastofest\n",
      "    False\n",
      "Failed to vectorize: truckfestival‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/vyuvn4lkh7\n",
      "    False\n",
      "Failed to vectorize: //t.co/a2zmiylhde\n",
      "    False\n",
      "Failed to vectorize: millionair_mag\n",
      "    False\n",
      "Failed to vectorize: //t.co/e17pzeula3\n",
      "    False\n",
      "Failed to vectorize: inspirationquotes‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/e9lf9z0v91\n",
      "    False\n",
      "Failed to vectorize: olvid√©\n",
      "    False\n",
      "Failed to vectorize: //t.co/ybwnidifgl\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/kmoog6ehlm\n",
      "    False\n",
      "Failed to vectorize: //t.co/ntgyu8juzm\n",
      "    False\n",
      "Failed to vectorize: mitrostziavaras\n",
      "    False\n",
      "Failed to vectorize: flowerstagram\n",
      "    False\n",
      "Failed to vectorize: flowerstyles_gf\n",
      "    False\n",
      "Failed to vectorize: mitros1973\n",
      "    False\n",
      "Failed to vectorize: igers_greece\n",
      "    False\n",
      "Failed to vectorize: //t.co/oghzkid9rq\n",
      "    False\n",
      "Failed to vectorize: leftandright\n",
      "    False\n",
      "Failed to vectorize: //t.co/sqrt1bosqv\n",
      "    False\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/garu0pjh9f\n",
      "    False\n",
      "Failed to vectorize: //t.co/nnvp3osnij\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: christmasdecorations‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/6c49x7gyhj\n",
      "    False\n",
      "Failed to vectorize: fugazza.\n",
      "    False\n",
      "Failed to vectorize: gaiafugazza\n",
      "    False\n",
      "Failed to vectorize: hrm199_ltd\n",
      "    False\n",
      "Failed to vectorize: //t.co/0ziupqoloh\n",
      "    False\n",
      "Failed to vectorize: üßü‚Äç‚ôÇÔ∏èüßü‚Äç‚ôÇÔ∏è\n",
      "    True\n",
      "Failed to vectorize: photooftheday\n",
      "    False\n",
      "Failed to vectorize: picoftheday\n",
      "    False\n",
      "Failed to vectorize: //t.co/fo5itlwlfj\n",
      "    False\n",
      "Failed to vectorize: //t.co/hs0rrqsi1d\n",
      "    False\n",
      "Failed to vectorize: philippinesüáµüá≠\n",
      "    True\n",
      "Failed to vectorize: wegettoserve\n",
      "    False\n",
      "Failed to vectorize: //t.co/krxnl89jlv\n",
      "    False\n",
      "Failed to vectorize: //t.co/e4nw86unz1\n",
      "    False\n",
      "Failed to vectorize: //t.co/lf83yx3zqj\n",
      "    False\n",
      "Failed to vectorize: lastminutegifts\n",
      "    False\n",
      "Failed to vectorize: lastminuteshopping\n",
      "    False\n",
      "Failed to vectorize: useyourtimewisely\n",
      "    False\n",
      "Failed to vectorize: dontgetsuckedin\n",
      "    False\n",
      "Failed to vectorize: //t.co/r1cmlquty8\n",
      "    False\n",
      "Failed to vectorize: sabrinajcarroll\n",
      "    False\n",
      "Failed to vectorize: //t.co/hahwrbllun\n",
      "    False\n",
      "Failed to vectorize: redcircletravel\n",
      "    False\n",
      "Failed to vectorize: londonüá¨üáß\n",
      "    True\n",
      "Failed to vectorize: tagforlikes\n",
      "    False\n",
      "Failed to vectorize: like4like‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/1ueqcallbk\n",
      "    False\n",
      "Failed to vectorize: lilliawalsh\n",
      "    False\n",
      "Failed to vectorize: //t.co/u2t4bvmckt\n",
      "    False\n",
      "Failed to vectorize: fw_photo\n",
      "    False\n",
      "Failed to vectorize: lovelululondon\n",
      "    False\n",
      "Failed to vectorize: shelleysumnerhair\n",
      "    False\n",
      "Failed to vectorize: charlottefitzjohnmua\n",
      "    False\n",
      "Failed to vectorize: milkmodelmanagement\n",
      "    False\n",
      "Failed to vectorize: //t.co/gc3gqkbq8x\n",
      "    False\n",
      "Failed to vectorize: //t.co/zlcmyiyze6\n",
      "    False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to vectorize: beyourownkindofbeautiful\n",
      "    False\n",
      "Failed to vectorize: givelovetogetlove\n",
      "    False\n",
      "Failed to vectorize: flamingomediapr\n",
      "    False\n",
      "Failed to vectorize: //t.co/wzkqehuex8\n",
      "    False\n",
      "Failed to vectorize: //t.co/ewj3exsttn\n",
      "    False\n",
      "Failed to vectorize: homesweethome\n",
      "    False\n",
      "Failed to vectorize: //t.co/mpqhsu8fzq\n",
      "    False\n",
      "Failed to vectorize: ladysingstheblues‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\n",
      "    False\n",
      "Failed to vectorize: //t.co/xlidpdsiyv\n",
      "    False\n",
      "Failed to vectorize: halfwaytoheaven\n",
      "    False\n",
      "Failed to vectorize: //t.co/aio12tb0cp\n",
      "    False\n",
      "Failed to vectorize: anyguycanbeanyguy\n",
      "    False\n",
      "Failed to vectorize: //t.co/i4jsi7z4xr\n",
      "    False\n",
      "Failed to vectorize: christmasiscoming\n",
      "    False\n",
      "Failed to vectorize: christmasdecorations‚Ä¶\n",
      "    False\n",
      "Failed to vectorize: //t.co/eafjwqayub\n",
      "    False\n"
     ]
    }
   ],
   "source": [
    "tweetnum = 100\n",
    "\n",
    "tbwTokenizer = TreebankWordTokenizer()\n",
    "glove50Model = gensimapi.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "tokenizedText = []\n",
    "untokenizedText = \"\"\n",
    "tknTextVector = []\n",
    "\n",
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "\n",
    "def break_compound_word(compound_word,model):\n",
    "    possible_words = []\n",
    "    first_word=\"\"\n",
    "    for i,xchar in enumerate(compound_word):\n",
    "        first_word+=xchar\n",
    "        try:\n",
    "            model[first_word]\n",
    "            second_word = compound_word[i+1:] \n",
    "            model[second_word]\n",
    "            possible_words.append([first_word,second_word])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return possible_words\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\"#love\",geocode=\"51.5098,-0.1180,1km\",tweet_mode=\"extended\").items(tweetnum):\n",
    "    tweet_text = tweet._json[\"full_text\"].lower()\n",
    "    tweet_text_tokenized = tbwTokenizer.tokenize(tweet_text)\n",
    "    tokenizedText.append(tweet_text_tokenized)\n",
    "    tknVectorList = []\n",
    "    \n",
    "    for text in tweet_text_tokenized:\n",
    "        try:\n",
    "            wordVector = glove50Model[text]\n",
    "            tknVectorList.append(wordVector)\n",
    "        except:\n",
    "            solutionFound = False\n",
    "            if(text[-1] == \".\" or text[-2:-1] == \"..\" or text[-3:-1]==\"...\"):\n",
    "                try:\n",
    "                    try:\n",
    "                        try:\n",
    "                            ##Try getting words up o three ...\n",
    "                            wordVector = glove50Model[text[:-3]]\n",
    "                            tknVectorList.append(wordVector)\n",
    "                            solutionFound = True\n",
    "                        except:\n",
    "                            pass\n",
    "                        wordVector = glove50Model[text[:-2]]\n",
    "                        tknVectorList.append(wordVector)\n",
    "                        solutionFound = True\n",
    "                    except:\n",
    "                        pass\n",
    "                    wordVector = glove50Model[text[:-1]]\n",
    "                    tknVectorList.append(wordVector)\n",
    "                    solutionFound = True\n",
    "                except:\n",
    "                    pass\n",
    "            if(text[-1] == \"‚Ä¶\"):\n",
    "                try:\n",
    "                    wordVector = glove50Model[text[:-1]]\n",
    "                    tknVectorList.append(wordVector)\n",
    "                    solutionFound=True\n",
    "                except:\n",
    "                    pass\n",
    "            if(has_emoji(text)):\n",
    "                ##Try extracting text from strings containing both text and strings\n",
    "                em_split_emoji = emoji.get_emoji_regexp().split(text)\n",
    "                em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                for eachEmoji in em_split:\n",
    "                    try:\n",
    "                        emojiDetail = ud.name(eachEmoji)\n",
    "                        emojiTokenized = tbwTokenizer.tokenize(emojiDetail.lower())\n",
    "                        emojiWordVectors = []  \n",
    "                        for descr in emojiTokenized:\n",
    "                            emojiVector = glove50Model[descr]\n",
    "                            emojiWordVectors.append(emojiVector)\n",
    "                            \n",
    "                        tknVectorList.extend(emojiWordVectors)\n",
    "                        solutionFound = True\n",
    "                    except:\n",
    "                        pass\n",
    "            child_words = break_compound_word(text,glove50Model)\n",
    "            ##Try upto 5 combined words\n",
    "            highestfrq = 0\n",
    "            if(len(child_words)!= 0):\n",
    "                for child_word_set in child_words:\n",
    "                    both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                    if (both_word_freq > highestfrq):\n",
    "                        most_likely_combo = child_word_set\n",
    "                        highestfrq = both_word_freq\n",
    "            \n",
    "                tknVectorList.append(glove50Model[most_likely_combo[0]])\n",
    "                tknVectorList.append(glove50Model[most_likely_combo[1]])\n",
    "                solutionFound = True\n",
    "            \n",
    "            if(not solutionFound):\n",
    "                print(\"Failed to vectorize: \" + text)\n",
    "                print(\"    \" + str(has_emoji(text)))\n",
    "                    \n",
    "    tknTextVector.extend(tknVectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:50:06.604356Z",
     "start_time": "2019-12-28T15:50:06.594345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['üíØ', 'üôåüèΩ', '‚ù§', 'Ô∏è']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def has_emoji(s):\n",
    "    em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "    emojiExists = False\n",
    "    for emojiTest in em_split:\n",
    "        if(emojiTest in UNICODE_EMOJI):\n",
    "            emojiExists = True\n",
    "    \n",
    "    return emojiExists\n",
    "\n",
    "stringtest = \"üíØüôåüèΩ‚ù§Ô∏è\"\n",
    "\n",
    "em_split_emoji = emoji.get_emoji_regexp().split(stringtest)\n",
    "em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "\n",
    "em_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T15:56:27.936352Z",
     "start_time": "2019-12-28T15:56:27.930200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HUNDRED POINTS SYMBOL'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ud.name(\"üíØ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T05:36:28.979265Z",
     "start_time": "2019-12-29T05:36:25.657474Z"
    }
   },
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T05:44:12.906674Z",
     "start_time": "2019-12-29T05:36:41.466018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: /Users/tamimazmain/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: /Users/tamimazmain/stanfordnlp_resources/en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 235M/235M [07:09<00:00, 546kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/tamimazmain/stanfordnlp_resources/en_ewt_models.zip\n",
      "Extracting models file for: en_ewt\n",
      "Cleaning up...Done.\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/Users/tamimazmain/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "('Barack', '4', 'nsubj:pass')\n",
      "('Obama', '1', 'flat')\n",
      "('was', '4', 'aux:pass')\n",
      "('born', '0', 'root')\n",
      "('in', '6', 'case')\n",
      "('Hawaii', '4', 'obl')\n",
      "('.', '4', 'punct')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')   # This downloads the English models for the neural pipeline\n",
    "nlp = stanfordnlp.Pipeline() # This sets up a default neural pipeline in English\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T06:58:23.990256Z",
     "start_time": "2019-12-29T06:58:23.875112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stanfordnlp.pipeline.doc.Sentence at 0x1a2295b828>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc = nlp([\"Barack\",\"Obama\",\"was\", \"born\", \"in\",\"Hawaii\",\".\",\"He\",\"was\",\"elected\",\"president\",\"in\",\"2008\",\".\"])\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.  He was elected president in 2008.\")\n",
    "doc.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T07:27:47.269874Z",
     "start_time": "2019-12-29T07:27:47.260681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tknTextVector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T04:02:24.782415Z",
     "start_time": "2019-12-30T04:02:24.725634Z"
    },
    "code_folding": [
     2,
     20,
     39,
     44,
     47,
     50,
     90,
     112
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deathbed', 'whyso', 'thedog']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class HashtagDetailsExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns a list of tweets (in text form) from the given hashtag\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,tag):\n",
    "        tweet_text_list = []\n",
    "        for tweet in tweepy.Cursor(api.search,q=tag,geocode=\"51.5098,-0.1180,1km\",tweet_mode=\"extended\").items(tweetnum):\n",
    "            tweet_text_list.append(tweet._json[\"full_text\"].lower())\n",
    "        \n",
    "        return tweet_text_list\n",
    "   \n",
    "class Tokenizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Returns tokenized text with the supplied Tokenizer \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,tokenizer_model,lowercase=True):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "        self.lowercase = lowercase\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        tokenized_tweets = []\n",
    "        for tweet in X:    \n",
    "            tweet_text_tokenized = self.tokenizer_model.tokenize(tweet)\n",
    "            tokenized_tweets.append(tweet_text_tokenized)\n",
    "        return tokenized_tweets\n",
    "\n",
    "class ConvertEmojis(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts the description of the emoji and repalces it with the emoji itself\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer_model):\n",
    "        self.tokenizer_model = tokenizer_model\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def hasemoji(self,s):\n",
    "        em_split_emoji = emoji.get_emoji_regexp().split(s)\n",
    "        em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "        em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "        emojiExists = False\n",
    "        for emojiTest in em_split:\n",
    "            if(emojiTest in UNICODE_EMOJI):\n",
    "                emojiExists = True\n",
    "    \n",
    "        return emojiExists\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "#             print(X[i])\n",
    "            for ii,word_token in enumerate(tweet):\n",
    "                if(self.hasemoji(word_token)):\n",
    "                    em_split_emoji = emoji.get_emoji_regexp().split(word_token)\n",
    "                    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "                    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "                    emoji_detail_tokenized = []\n",
    "                    for each_emoji in em_split:\n",
    "                        try:\n",
    "                            emoji_detail = ud.name(each_emoji)\n",
    "                            emoji_tokenized = self.tokenizer_model.tokenize(emoji_detail.lower())                \n",
    "                            emoji_detail_tokenized.extend(emoji_tokenized)\n",
    "                        except:\n",
    "#                             print(\"Attempted: \" + word_token)\n",
    "#                             print(\"Failed: \" + each_emoji + \" in:\")\n",
    "#                             print(emoji_tokenized)\n",
    "                            print(\"FAIL\")\n",
    "#                             print(each_emoji)\n",
    "                    print(\"Converted \" + word_token + \" to:\")\n",
    "                    print(emoji_detail_tokenized)\n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + emoji_detail_tokenized + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(emoji_detail_tokenized) - 1\n",
    "        return X_copy\n",
    "\n",
    "class RemoveTrailingPeriods(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X ,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X,y=None):\n",
    "        for i,tweet in enumerate(X):\n",
    "            indexes_with_period = []\n",
    "            for ii, word_token in enumerate(tweet):\n",
    "                while(X[i][ii][-1]==\".\" and len(X[i][ii]) != 1):\n",
    "                    X[i][ii] = X[i][ii][:-1]\n",
    "                \n",
    "                if(X[i][ii][-1]==\"‚Ä¶\"):\n",
    "                    X[i][ii] = X[i][ii][:-1]\n",
    "                    \n",
    "        \n",
    "                    \n",
    "        return X\n",
    "                \n",
    "class RemovePunctuation(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,charsequence):\n",
    "        self.charsequence = set(charsequence)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for i,tweet in enumerate(X):\n",
    "            \n",
    "            X_copy[i] = [text for text in tweet if not set([text]).issubset(self.charsequence)]\n",
    "            \n",
    "        return X_copy\n",
    "\n",
    "    \n",
    "class BreakupWords(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def break_compound_word(self,compound_word):\n",
    "        possible_words = []\n",
    "        first_word=\"\"\n",
    "        for i,xchar in enumerate(compound_word):\n",
    "            first_word+=xchar\n",
    "            try:\n",
    "                print(\"first_word\")\n",
    "                self.model[first_word]\n",
    "                second_word = compound_word[i+1:]\n",
    "                self.model[second_word]\n",
    "                possible_words.append([first_word,second_word])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return possible_words\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        for i,tweet in enumerate(X):\n",
    "            shiftindex = 0\n",
    "            for ii, token in enumerate(tweet):\n",
    "                child_words = self.break_compound_word(text)\n",
    "                ##Try upto 5 combined words\n",
    "                highestfrq = 0\n",
    "                if(len(child_words)!= 0):\n",
    "                    for child_word_set in child_words:\n",
    "                        both_word_freq = word_frequency(child_word_set[0],\"en\")*word_frequency(child_word_set[1],\"en\")\n",
    "                        if (both_word_freq > highestfrq):\n",
    "                            most_likely_combo = child_word_set\n",
    "                            highestfrq = both_word_freq\n",
    "                    \n",
    "                    X_copy[i].pop(ii + shiftindex)\n",
    "                    X_copy[i] = X_copy[i][:ii + shiftindex] + most_likely_combo + X_copy[i][ii + shiftindex:]\n",
    "                    shiftindex += len(most_likely_combo) - 1\n",
    "                    \n",
    "        return X_copy\n",
    "                    \n",
    "        \n",
    "    \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# treeBankTokenizer = RegexpTokenizer(r'\\w+')\n",
    "# treeBankTokenizer = TreebankWordTokenizer()\n",
    "# pipelineTokenizer = Tokenizer(tokenizer_model=treeBankTokenizer)\n",
    "# pipelineTagExtractor = HashtagDetailsExtractor()\n",
    "# pipelinePeriod = RemoveTrailingPeriods()\n",
    "# pipelineConvEmoji = ConvertEmojis(tokenizer_model=treeBankTokenizer)\n",
    "\n",
    "# tokenized_tweets = pipelineTokenizer.fit_transform(pipelineTagExtractor.fit_transform(\"#love\"))\n",
    "# noperiod = pipelinePeriod.fit_transform(tokenized_tweets)\n",
    "\n",
    "# emoji_reduced = pipelineConvEmoji.fit_transform(noperiod)\n",
    "\n",
    "# pipelineRmPunc = RemovePunctuation([\".\",\"#\",\":\",\"‚Ä¢\",\",\",\"@\",\"\\\"\",\";\",\"\\'\"])\n",
    "\n",
    "# punc_reduced = pipelineRmPunc.fit_transform(emoji_reduced)\n",
    "\n",
    "# punc_reduced\n",
    "\n",
    "word_set = [[\"deathbed\",\"whyso\",\"thedog\"]]\n",
    "# def break_compound_word1(compound_word,modell):\n",
    "#     possible_words = []\n",
    "#     first_word=\"\"\n",
    "#     for i,xchar in enumerate(compound_word):\n",
    "#         first_word+=xchar\n",
    "#         try:\n",
    "#             modell[first_word]\n",
    "#             second_word = compound_word[i+1:] \n",
    "#             modell[second_word]\n",
    "#             possible_words.append([first_word,second_word])\n",
    "#         except:\n",
    "#             pass\n",
    "        \n",
    "#     return possible_words\n",
    "\n",
    "children = break_compound_word1(\"thedog\",glove50Model)\n",
    "\n",
    "pipelineBrkWords = BreakupWords(glove50Model)\n",
    "pipelineBrkWords.fit_transform(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T11:40:22.563473Z",
     "start_time": "2019-12-29T11:40:22.558449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T13:11:42.746178Z",
     "start_time": "2019-12-29T13:11:42.739081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.58402 ,  0.39031 ,  0.65282 , -0.3403  ,  0.19493 , -0.83489 ,\n",
       "        0.11929 , -0.57291 , -0.56844 ,  0.72989 , -0.56975 ,  0.53436 ,\n",
       "       -0.38034 ,  0.22471 ,  0.98031 , -0.2966  ,  0.126   ,  0.55222 ,\n",
       "       -0.62737 , -0.082242, -0.085359,  0.31515 ,  0.96077 ,  0.31986 ,\n",
       "        0.87878 , -1.5189  , -1.7831  ,  0.35639 ,  0.9674  , -1.5497  ,\n",
       "        2.335   ,  0.8494  , -1.2371  ,  1.0623  , -1.4267  , -0.49056 ,\n",
       "        0.85465 , -1.2878  ,  0.60204 , -0.35963 ,  0.28586 , -0.052162,\n",
       "       -0.50818 , -0.63459 ,  0.33889 ,  0.28416 , -0.2034  , -1.2338  ,\n",
       "        0.46715 ,  0.78858 ], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove50Model[\"!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T08:36:47.098176Z",
     "start_time": "2019-12-29T08:36:47.088570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 2, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "# this is a very toy example, do not try this at home unless you want to understand the usage differences\n",
    "docs=[\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"\n",
    "     ]\n",
    "  \n",
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "\n",
    "word_count_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T08:31:10.104417Z",
     "start_time": "2019-12-29T08:31:10.096190Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T10:50:01.237692Z",
     "start_time": "2019-12-29T10:50:01.231849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'k', 'l', 'x', 'j']\n",
      "['a', 'k', 'l', 'x', 'j']\n",
      "['a', 'aa', 'cc', 'zz', 'k', 'l', 'x', 'j']\n"
     ]
    }
   ],
   "source": [
    "ii = 1\n",
    "shiftindex = 0\n",
    "XXL = [\"a\", \"b\" , \"k\", \"l\", \"x\" , \"j\"]\n",
    "print(XXL)\n",
    "XXL.pop(ii + shiftindex)\n",
    "print(XXL)\n",
    "emoji_detail_tokenized = [\"aa\", \"cc\",\"zz\"]\n",
    "XXL = XXL[:ii + shiftindex] + emoji_detail_tokenized + XXL[ii + shiftindex:]\n",
    "# shiftindex += len(emoji_detail_tokenized) - 1\n",
    "\n",
    "print(XXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T10:58:00.240686Z",
     "start_time": "2019-12-29T10:58:00.235087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'cc', 'zz', 'k', 'l', 'x', 'j']\n",
      "['a', 'aa', 'cc', 'zz', 'l', 'x', 'j']\n",
      "['a', 'aa', 'cc', 'zz', 'jj', 'kk', 'pp', 'l', 'x', 'j']\n"
     ]
    }
   ],
   "source": [
    "ii = 2\n",
    "shiftindex = 2\n",
    "XXL = ['a', 'aa', 'cc', 'zz', 'k', 'l', 'x', 'j']\n",
    "print(XXL)\n",
    "XXL.pop(ii + shiftindex)\n",
    "print(XXL)\n",
    "emoji_detail_tokenized = [\"jj\", \"kk\",\"pp\"]\n",
    "XXL = XXL[:ii + shiftindex] + emoji_detail_tokenized + XXL[ii + shiftindex:]\n",
    "# shiftindex += len(emoji_detail_tokenized) - 1\n",
    "\n",
    "print(XXL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T12:55:16.921996Z",
     "start_time": "2019-12-29T12:55:16.916360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xff\\xfe>\\xd8p\\xdd'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import unicodedata\n",
    "# title=\"ü•∞\"\n",
    "# unicodedata.normalize('NFKD', title).encode('ascii','ignore')\n",
    "\n",
    "\"ü•∞\".encode(\"utf-16\",\"strict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:40:28.877883Z",
     "start_time": "2019-12-29T16:40:28.825072Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "name() argument 1 must be a unicode character, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-209-c9d83455ff8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ò∫Ô∏è\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: name() argument 1 must be a unicode character, not str"
     ]
    }
   ],
   "source": [
    "ud.name(\"‚ò∫Ô∏è\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T14:12:59.858769Z",
     "start_time": "2019-12-29T14:12:59.851927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sett = set([\"#\",\".\",\"k\"])\n",
    "\n",
    "sett_1 = set([\"#\"])\n",
    "\n",
    "sett_1.issubset(sett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T15:37:19.966115Z",
     "start_time": "2019-12-29T15:37:19.947762Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'don't' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-f488d82b9793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove50Model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"don't\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/hashrecf/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'don't' not in vocabulary\""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T16:25:57.534358Z",
     "start_time": "2019-12-29T16:25:57.521470Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-206-b4cdd51867c6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-206-b4cdd51867c6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    add(sample+=1,2)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def add(num1,num2):\n",
    "    return num1 + num2\n",
    "\n",
    "sample = 10\n",
    "add(sample+=1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
